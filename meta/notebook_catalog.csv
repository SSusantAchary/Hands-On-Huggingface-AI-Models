section,model_id,model_name,use_case,library_deps,hardware_profile,ram_usage_estimate,notes,notebook_path,colab_link,status
nlp,bert-base-uncased,BERT base uncased,Sentiment/IMDB baseline (CPU-first pipeline + LoRA TODO),"transformers,datasets,evaluate",CPU/GPU/MLX,<4GB,Fast CPU; great starter; add LoRA later; license review: Apache-2.0,nlp/nlp_notebooks/nlp-01-bert-base-uncased.ipynb,TODO,planned
nlp,sentence-transformers/all-MiniLM-L6-v2,all-MiniLM-L6-v2,Semantic search embeddings (vector DB ready),"sentence-transformers,transformers",CPU/GPU/MLX,<4GB,Good default for retrieval; multilingual alt available,nlp/nlp_notebooks/nlp-02-all-minilm-l6-v2.ipynb,TODO,planned
nlp,roberta-base,RoBERTa base,Sentiment on social reviews baseline,"transformers,datasets,evaluate",CPU/GPU/MLX,4–8GB,Solid base; add class weighting; license: MIT,nlp/nlp_notebooks/nlp-03-roberta-base.ipynb,TODO,planned
nlp,distilbert-base-uncased,DistilBERT base uncased,Real-time sentiment microservice,"transformers,datasets",CPU/GPU/MLX,<4GB,Great for quantization tests; low latency,nlp/nlp_notebooks/nlp-04-distilbert-base-uncased.ipynb,TODO,planned
nlp,deepset/roberta-base-squad2,RoBERTa QA SQuAD2,Extractive QA over policy docs,"transformers,datasets",CPU/GPU/MLX,4–8GB,Chunk long inputs; add citation spans,nlp/nlp_notebooks/nlp-05-roberta-qa-squad2.ipynb,TODO,planned
nlp,t5-small,T5 Small,Abstractive summarization quickstart,"transformers,datasets,evaluate",CPU/GPU/MLX,4–8GB,CPU okay with batch=1; add ROUGE eval,nlp/nlp_notebooks/nlp-06-t5-small.ipynb,TODO,planned
nlp,facebook/bart-base,BART base,News summarization baseline,"transformers,datasets,evaluate",CPU/GPU,8–16GB,Prefer GPU for beams; license: MIT,nlp/nlp_notebooks/nlp-07-bart-base.ipynb,TODO,planned
nlp,google/flan-t5-base,FLAN-T5 base,Instruction following snippets,transformers,CPU/GPU,8–16GB,Great with prompt templates; add safety note,nlp/nlp_notebooks/nlp-08-flan-t5-base.ipynb,TODO,planned
nlp,Helsinki-NLP/opus-mt-en-hi,OPUS-MT EN-HI,En↔Hi translation starter,transformers,CPU/GPU,4–8GB,Download tokenizer offline; quality mid-tier,nlp/nlp_notebooks/nlp-09-opus-mt-en-hi.ipynb,TODO,planned
nlp,facebook/mbart-large-50-many-to-many-mmt,mBART-50 M2M,Multilingual translation playground,transformers,CPU/GPU,8–16GB,Needs sentencepiece; license review: CC-BY-NC,nlp/nlp_notebooks/nlp-10-mbart-50-m2m.ipynb,TODO,planned
nlp,intfloat/e5-base-v2,E5 Base v2,Dense retrieval embedding pipeline,"transformers,datasets",CPU/GPU/MLX,4–8GB,Pair with FAISS; bilingual ready,nlp/nlp_notebooks/nlp-11-e5-base-v2.ipynb,TODO,planned
nlp,BAAI/bge-small-en,BGE Small EN,Search embeddings lightweight,transformers,CPU/GPU/MLX,<4GB,CPU friendly; add normalization step,nlp/nlp_notebooks/nlp-12-bge-small-en.ipynb,TODO,planned
nlp,jinaai/jina-embeddings-v2-base-en,Jina Embeddings Base,Hybrid retrieval embeddings,transformers,CPU/GPU,4–8GB,Enable batching for speed; license review: MIT,nlp/nlp_notebooks/nlp-13-jina-embeddings-base.ipynb,TODO,planned
nlp,mixedbread-ai/mxbai-rerank-xsmall-v1,MXBAI Rerank XSmall,Cross-encoder reranking for search,transformers,CPU/GPU,4–8GB,Use top-k=50; consider int8,nlp/nlp_notebooks/nlp-14-mxbai-rerank-xsmall.ipynb,TODO,planned
nlp,deepset/gbert-base,German BERT base,German NER for support tickets,transformers,CPU/GPU,4–8GB,Requires cased text; license review: MIT,nlp/nlp_notebooks/nlp-15-german-bert-base.ipynb,TODO,planned
nlp,microsoft/deberta-v3-base,DeBERTa v3 base,Intent classification upgrades,"transformers,datasets",CPU/GPU,8–16GB,Better accuracy; needs ONNX export tip,nlp/nlp_notebooks/nlp-16-deberta-v3-base.ipynb,TODO,planned
nlp,sentence-transformers/paraphrase-multilingual-mpnet-base-v2,Paraphrase MPNet Multilingual,Paraphrase detection multilingual,"sentence-transformers,transformers",CPU/GPU/MLX,4–8GB,Great for semantic dedupe; license: Apache-2.0,nlp/nlp_notebooks/nlp-17-paraphrase-mpnet-multilingual.ipynb,TODO,planned
nlp,google/gemma-2b-it,Gemma 2B IT,Compact instruction chat,transformers,CPU/GPU,8–16GB,Runs on 12GB GPU; Gemma license review,nlp/nlp_notebooks/nlp-18-gemma-2b-it.ipynb,TODO,planned
nlp,mistralai/Mistral-7B-Instruct-v0.2,Mistral 7B Instruct,Advanced instruction generation,transformers,CPU/GPU,32GB+,Use 4-bit quant on GPU; Apache-2.0,nlp/nlp_notebooks/nlp-19-mistral-7b-instruct.ipynb,TODO,planned
nlp,Alibaba-NLP/gte-base,GTE Base,Dual-encoder retrieval baseline,transformers,CPU/GPU/MLX,4–8GB,Normalize embeddings; multilingual ready,nlp/nlp_notebooks/nlp-20-gte-base.ipynb,TODO,planned
vision,google/vit-base-patch16-224,ViT Base 224,Image classification baseline (Imagenette),"transformers,datasets,timm",CPU/GPU/MLX,4–8GB,Simple fine-tune; CPU okay; MPS expected,vision/vision_notebooks/vision-01-vit-base-224.ipynb,TODO,planned
vision,facebook/detr-resnet-50,DETR ResNet-50,Object detection on sample images,"transformers,torchvision",CPU/GPU,8–16GB,GPU recommended; CPU slow; MPS expected; check license,vision/vision_notebooks/vision-02-detr-resnet-50.ipynb,TODO,planned
vision,microsoft/resnet-50,ResNet-50,Classic classification transfer,"torchvision,timm",CPU/GPU/MLX,4–8GB,Great for benchmarking augmentations,vision/vision_notebooks/vision-03-resnet-50.ipynb,TODO,planned
vision,laion/CLIP-ViT-B-32-laion2B-s34B-b79K,CLIP ViT-B LAION,Zero-shot retrieval large corpus,transformers,CPU/GPU,4–8GB,Needs fp16 on GPU; license review: LAION,vision/vision_notebooks/vision-04-clip-vit-b-laion.ipynb,TODO,planned
vision,openai/clip-vit-large-patch14,CLIP ViT-L/14,High-accuracy zero-shot classification,transformers,CPU/GPU,8–16GB,Heavy on CPU; prefer GPU; license review,vision/vision_notebooks/vision-05-clip-vit-l-14.ipynb,TODO,planned
vision,nvidia/segformer-b0-finetuned-ade-512-512,SegFormer B0 ADE,Semantic segmentation quickstart,transformers,CPU/GPU,8–16GB,Resize inputs carefully; MIT license,vision/vision_notebooks/vision-06-segformer-b0-ade.ipynb,TODO,planned
vision,microsoft/conditional-detr-resnet-50,Conditional DETR,Detection with conditional queries,"transformers,torchvision",CPU/GPU,8–16GB,Tune for small objects; add eval script,vision/vision_notebooks/vision-07-conditional-detr.ipynb,TODO,planned
vision,facebook/dpt-hybrid-midas,DPT Hybrid MiDaS,Monocular depth estimation,transformers,CPU/GPU/MLX,8–16GB,Use fp16 on GPU; good for robotics,vision/vision_notebooks/vision-08-dpt-hybrid-midas.ipynb,TODO,planned
vision,intel/dpt-large,DPT Large,High quality depth maps,transformers,CPU/GPU,16–32GB,Memory heavy; crop inputs; Apache-2.0,vision/vision_notebooks/vision-09-dpt-large.ipynb,TODO,planned
vision,microsoft/trocr-base-printed,TrOCR Base Printed,Printed OCR pipeline,"transformers,datasets",CPU/GPU,4–8GB,Needs pillow + sentencepiece; MIT,vision/vision_notebooks/vision-10-trocr-base-printed.ipynb,TODO,planned
vision,microsoft/dit-base-finetuned-ocr,DiT OCR Base,Document OCR with layout,transformers,CPU/GPU,8–16GB,Enable fp16; license review: MIT,vision/vision_notebooks/vision-11-dit-ocr-base.ipynb,TODO,planned
vision,facebook/sam-vit-base,SAM ViT Base,Segment anything interactive,transformers,CPU/GPU,16–32GB,Large prompts; license review: SAM,vision/vision_notebooks/vision-12-sam-vit-base.ipynb,TODO,planned
vision,IDEA-Research/grounding-dino-tiny,Grounding DINO Tiny,Grounded object detection,transformers,CPU/GPU,8–16GB,Requires grounding tokens; license review: Apache-2.0,vision/vision_notebooks/vision-13-grounding-dino-tiny.ipynb,TODO,planned
vision,apple/mobilevit-small,MobileViT Small,Edge image classification,transformers,CPU/GPU/MLX,<4GB,Great on mobile; MIT license,vision/vision_notebooks/vision-14-mobilevit-small.ipynb,TODO,planned
vision,facebook/mask2former-swin-base-ade20k-semantic,Mask2Former Swin Base,Semantic segmentation advanced,transformers,CPU/GPU,16–32GB,Prefer GPU; add sliding window,vision/vision_notebooks/vision-15-mask2former-swin-base.ipynb,TODO,planned
vision,ultralytics/yolov8n,YOLOv8n,Real-time detection baseline,"ultralytics,torchvision",CPU/GPU/MLX,4–8GB,Export to ONNX easily; AGPL license review,vision/vision_notebooks/vision-16-yolov8n.ipynb,TODO,planned
vision,microsoft/beit-base-patch16-224-pt22k-ft22k,BEiT Base,Vision transformer fine-tuning,transformers,CPU/GPU,8–16GB,Strong baseline; needs data aug,vision/vision_notebooks/vision-17-beit-base.ipynb,TODO,planned
vision,facebook/convnextv2-tiny-1k-224,ConvNeXtV2 Tiny,Modern convnet baseline,transformers,CPU/GPU,4–8GB,Fast inference; record top-1,vision/vision_notebooks/vision-18-convnextv2-tiny.ipynb,TODO,planned
vision,google/depth-anything-small-hf,Depth Anything Small,Fast monocular depth,transformers,CPU/GPU,4–8GB,Runs on MPS; great for AR,vision/vision_notebooks/vision-19-depth-anything-small.ipynb,TODO,planned
vision,facebook/dinov2-base,DINOv2 Base,Self-supervised feature extractor,transformers,CPU/GPU,8–16GB,Use for retrieval; MIT license,vision/vision_notebooks/vision-20-dinov2-base.ipynb,TODO,planned
vision,microsoft/trocr-base-handwritten,TrOCR Base Handwritten,Handwritten OCR pipeline,"transformers,datasets",CPU/GPU,4–8GB,Best on single-line crops; add beam search for cursive,vision/vision_notebooks/vision-21-trocr-base-handwritten.ipynb,TODO,planned
vision,microsoft/trocr-large-printed,TrOCR Large Printed,High-accuracy printed OCR,transformers,GPU,12–16GB,Large encoder-decoder; pair with tiling for long pages,vision/vision_notebooks/vision-22-trocr-large-printed.ipynb,TODO,planned
vision,microsoft/dit-large-finetuned-docvqa,DiT Large DocVQA,Layout-aware document QA,transformers,GPU,16–24GB,Memory heavy; requires mixed precision,vision/vision_notebooks/vision-23-dit-large-docvqa.ipynb,TODO,planned
vision,naver-clova-ix/donut-base-finetuned-docvqa,Donut Base DocVQA,OCR-free document understanding,"transformers,seq2seq",GPU,12–16GB,Sensitive to input resolution; ensure 960×720 sizing,vision/vision_notebooks/vision-24-donut-base-docvqa.ipynb,TODO,planned
vision,microsoft/table-transformer-structure-recognition,Table Transformer Structure,Table structure detection,transformers,CPU/GPU,8–16GB,Use with OCR models for cell-wise extraction,vision/vision_notebooks/vision-25-table-transformer-structure.ipynb,TODO,planned
vision,stabilityai/sd-turbo,SD Turbo,Fast SDXL distilled text-to-image,"diffusers,torch",GPU,6–8GB VRAM,Great for 2–4 step 512² renders; add refiner for detail,vision/vision_notebooks/vision-26-sd-turbo.ipynb,https://huggingface.co/docs/diffusers/using-diffusers/sd_turbo,planned
vision,stabilityai/stable-diffusion-xl-base-1.0,Stable Diffusion XL Base 1.0,Photoreal 1024² text-to-image,"diffusers,torch",GPU,12–16GB VRAM,Pair with SDXL refiner for best quality,vision/vision_notebooks/vision-27-sdxl-base.ipynb,https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers_doc/en/pytorch/sdxl.ipynb,planned
vision,stable-diffusion-v1-5/stable-diffusion-v1-5,Stable Diffusion v1.5,Classic 512² diffusion baseline,"diffusers,torch",GPU,6–8GB VRAM,Most LoRA/control assets target this checkpoint,vision/vision_notebooks/vision-28-sd15.ipynb,https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb,planned
vision,black-forest-labs/FLUX.1-dev,FLUX.1 Dev,Cinematic Flux text-to-image,"diffusers,torch",GPU,24GB VRAM,Bfloat16 recommended; requires diffusers ≥0.31,vision/vision_notebooks/vision-29-flux-dev.ipynb,https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/flux_with_cfg.ipynb,planned
vision,black-forest-labs/FLUX.1-schnell,FLUX.1 Schnell,Speed-optimized Flux variant,"diffusers,torch",GPU,16GB VRAM,Lower detail than Dev; excellent for rapid ideation,vision/vision_notebooks/vision-30-flux-schnell.ipynb,https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/flux_with_cfg.ipynb,planned
vision,lightx2v/Qwen-Image-Lightning,Qwen-Image Lightning,Multilingual text-to-image,"diffusers,torch",GPU,16–20GB VRAM,Lightning scheduler yields 2–4 step sampling,vision/vision_notebooks/vision-31-qwen-image-lightning.ipynb,https://huggingface.co/spaces/Qwen/Qwen2-Image,planned
vision,stabilityai/stable-diffusion-2-1,Stable Diffusion 2.1,Improved perspective diffusion,"diffusers,torch",GPU,8–12GB VRAM,Uses new tokenizer; better negatives than 1.x,vision/vision_notebooks/vision-32-sd21.ipynb,https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb,planned
vision,stabilityai/sdxl-turbo,SDXL Turbo,Turbo SDXL for real-time diffusion,"diffusers,torch",GPU,8–12GB VRAM,Best at 512–768²; license review required,vision/vision_notebooks/vision-33-sdxl-turbo.ipynb,https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers_doc/en/pytorch/sdxl_turbo.ipynb,planned
vision,CompVis/stable-diffusion-v1-4,Stable Diffusion v1.4,Original SD baseline,"diffusers,torch",GPU,6–8GB VRAM,Use with safety checker for public apps,vision/vision_notebooks/vision-34-sd14.ipynb,https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb,planned
vision,stabilityai/stable-diffusion-3-medium-diffusers,Stable Diffusion 3 Medium,Flow-matching text-to-image,"diffusers,torch",GPU,18–24GB VRAM,Requires `StableDiffusion3Pipeline` and diffusers ≥0.29,vision/vision_notebooks/vision-35-sd3-medium.ipynb,https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion_3,planned
audio,openai/whisper-tiny,Whisper Tiny,ASR on short clips (EN/multilingual),"transformers,torchaudio,soundfile",CPU/GPU/MLX,<4GB,Install ffmpeg; good CPU baseline,audio/audio_notebooks/audio-01-whisper-tiny.ipynb,TODO,planned
audio,facebook/wav2vec2-base-960h,Wav2Vec2 Base 960h,ASR baseline (LibriSpeech-style),"transformers,torchaudio",CPU/GPU/MLX,4–8GB,Works on CPU; add CTC decoding note,audio/audio_notebooks/audio-02-wav2vec2-base-960h.ipynb,TODO,planned
audio,openai/whisper-base,Whisper Base,Balanced ASR quality vs speed,"transformers,torchaudio,soundfile",CPU/GPU/MLX,4–8GB,Use beam size 1 on CPU; ffmpeg needed,audio/audio_notebooks/audio-03-whisper-base.ipynb,TODO,planned
audio,openai/whisper-small,Whisper Small,Improved ASR accuracy,"transformers,torchaudio,soundfile",CPU/GPU/MLX,8–16GB,GPU recommended; add language settings,audio/audio_notebooks/audio-04-whisper-small.ipynb,TODO,planned
audio,openai/whisper-medium,Whisper Medium,High accuracy ASR,"transformers,torchaudio,soundfile",CPU/GPU,16–32GB,Plan quantization; GPU strongly advised,audio/audio_notebooks/audio-05-whisper-medium.ipynb,TODO,planned
audio,jonatasgrosman/wav2vec2-large-xlsr-53-english,Wav2Vec2 XLSR EN,Multilingual ASR fine-tune,"transformers,torchaudio",CPU/GPU,8–16GB,Large memory; add vocab mapping,audio/audio_notebooks/audio-06-wav2vec2-xlsr-en.ipynb,TODO,planned
audio,facebook/hubert-large-ls960-ft,HuBERT Large,Self-supervised ASR feature extractor,"transformers,torchaudio",CPU/GPU,8–16GB,Great for finetuning; MIT license,audio/audio_notebooks/audio-07-hubert-large.ipynb,TODO,planned
audio,microsoft/wavlm-base-plus,WavLM Base Plus,Speech enhancement and ASR,"transformers,torchaudio",CPU/GPU,4–8GB,Use speechbrain recipes; MIT license,audio/audio_notebooks/audio-08-wavlm-base-plus.ipynb,TODO,planned
audio,microsoft/wavlm-large,WavLM Large,Speaker diarization features,"transformers,torchaudio",CPU/GPU,8–16GB,Heavy but accurate; add VAD tip,audio/audio_notebooks/audio-09-wavlm-large.ipynb,TODO,planned
audio,superb/hubert-base-superb-er,HuBERT Emotion,Emotion classification,"transformers,torchaudio",CPU/GPU,4–8GB,Requires torchaudio>=2.1; check labels,audio/audio_notebooks/audio-10-hubert-emotion.ipynb,TODO,planned
audio,superb/wav2vec2-base-superb-sid,Wav2Vec2 Speaker ID,Speaker identification,"transformers,torchaudio",CPU/GPU,4–8GB,Add enrollment pipeline; MIT,audio/audio_notebooks/audio-11-wav2vec2-speaker-id.ipynb,TODO,planned
audio,speechbrain/urbansound8k_ecapa,UrbanSound8K ECAPA,Urban sound classification,"speechbrain,torchaudio",CPU/GPU,4–8GB,Install ffmpeg; add data download step,audio/audio_notebooks/audio-12-urbansound8k-ecapa.ipynb,TODO,planned
audio,speechbrain/spkrec-ecapa-voxceleb,ECAPA VoxCeleb,Speaker verification embeddings,"speechbrain,torchaudio",CPU/GPU,4–8GB,Add score calibration tip,audio/audio_notebooks/audio-13-ecapa-voxceleb.ipynb,TODO,planned
audio,speechbrain/emotion-recognition-wav2vec2-IEMOCAP,Wav2Vec2 Emotion,Emotion recognition pipeline,"speechbrain,torchaudio",CPU/GPU,8–16GB,GPU improves speed; MIT license,audio/audio_notebooks/audio-14-wav2vec2-emotion.ipynb,TODO,planned
audio,speechbrain/google-speech-commands-cnn,Speech Commands CNN,Keyword spotting quickstart,"speechbrain,torchaudio",CPU/GPU/MLX,<4GB,Great edge baseline; add noise aug,audio/audio_notebooks/audio-15-speech-commands-cnn.ipynb,TODO,planned
audio,speechbrain/mtl-mimic-voicebank,MIMIC VoiceBank,Speech enhancement baseline,"speechbrain,torchaudio",CPU/GPU,4–8GB,Needs noise dataset; MIT license,audio/audio_notebooks/audio-16-mimic-voicebank.ipynb,TODO,planned
audio,microsoft/speecht5_tts,SpeechT5 TTS,Text-to-speech neural,"transformers,torchaudio",CPU/GPU,8–16GB,Requires HiFi-GAN vocoder; MIT license,audio/audio_notebooks/audio-17-speecht5-tts.ipynb,TODO,planned
audio,coqui/XTTS-v2,XTTS v2,Multi-speaker TTS,"TTS,torch",CPU/GPU,16–32GB,Check Coqui license; needs ffmpeg,audio/audio_notebooks/audio-18-xtts-v2.ipynb,TODO,planned
audio,espnet/kan-bayashi_ljspeech_vits,VITS LJSpeech,Fast TTS baseline,"espnet,torchaudio",CPU/GPU,8–16GB,Warm up for better quality; MIT,audio/audio_notebooks/audio-19-vits-ljspeech.ipynb,TODO,planned
audio,facebook/mms-tts-eng,MMS TTS EN,Multilingual MMS TTS,"transformers,torchaudio",CPU/GPU,8–16GB,License review: MMS; add phoneme note,audio/audio_notebooks/audio-20-mms-tts-en.ipynb,TODO,planned
multimodal,openai/clip-vit-base-patch32,CLIP ViT-B/32,Zero-shot image classification & retrieval,transformers,CPU/GPU/MLX,4–8GB,Popular baseline; add batching tip,multimodal/multimodal_notebooks/multimodal-01-clip-vit-b-32.ipynb,TODO,planned
multimodal,Salesforce/blip-image-captioning-base,BLIP Captioning Base,Image captioning on Flickr8k subset,"transformers,datasets",CPU/GPU,8–16GB,GPU recommended; CPU works small batch,multimodal/multimodal_notebooks/multimodal-02-blip-captioning-base.ipynb,TODO,planned
multimodal,Salesforce/blip2-opt-2.7b,BLIP2 OPT 2.7B,Image captioning with OPT decoder,transformers,CPU/GPU,16–32GB,Use bf16 on GPU; memory hungry,multimodal/multimodal_notebooks/multimodal-03-blip2-opt-2-7b.ipynb,TODO,planned
multimodal,laion/CLIP-ViT-L-14,CLIP ViT-L/14,High quality cross-modal retrieval,transformers,CPU/GPU,8–16GB,Use fp16; large checkpoints,multimodal/multimodal_notebooks/multimodal-04-clip-vit-l-14.ipynb,TODO,planned
multimodal,google/owlvit-base-patch32,OWL-ViT Base,Open vocabulary detection,transformers,CPU/GPU,8–16GB,Prompt carefully; MIT license,multimodal/multimodal_notebooks/multimodal-05-owl-vit-base.ipynb,TODO,planned
multimodal,google/owlvit-large-patch14,OWL-ViT Large,Open vocabulary detection high-accuracy,transformers,CPU/GPU,16–32GB,GPU highly recommended; MIT,multimodal/multimodal_notebooks/multimodal-06-owl-vit-large.ipynb,TODO,planned
multimodal,Salesforce/blip-vqa-base,BLIP VQA Base,Visual question answering,"transformers,datasets",CPU/GPU,8–16GB,Add answer post-process; MIT,multimodal/multimodal_notebooks/multimodal-07-blip-vqa-base.ipynb,TODO,planned
multimodal,Salesforce/instructblip-flan-t5-xl,InstructBLIP FLAN-T5 XL,Instruction tuned VQA,transformers,CPU/GPU,16–32GB,Use 4-bit to fit 16GB; license MIT,multimodal/multimodal_notebooks/multimodal-08-instructblip-flan-t5-xl.ipynb,TODO,planned
multimodal,mlfoundations/open_flamingo_3b,OpenFlamingo 3B,Few-shot multimodal generation,transformers,CPU/GPU,16–32GB,Requires CLIP vision tower; Apache-2.0,multimodal/multimodal_notebooks/multimodal-09-openflamingo-3b.ipynb,TODO,planned
multimodal,Qwen/Qwen2-VL-2B-Instruct,Qwen2 VL 2B,Compact VLM assistant,transformers,CPU/GPU,16–32GB,Quantize to 4-bit; license Qwen,multimodal/multimodal_notebooks/multimodal-10-qwen2-vl-2b.ipynb,TODO,planned
multimodal,BAAI/AltCLIP-L-m9,AltCLIP L-m9,Chinese-English CLIP,transformers,CPU/GPU,8–16GB,Great for cross-lingual retrieval,multimodal/multimodal_notebooks/multimodal-11-altclip-l-m9.ipynb,TODO,planned
multimodal,microsoft/kosmos-2-patch14-224,Kosmos-2,Vision language grounding,transformers,CPU/GPU,16–32GB,Needs caption prompts; MIT,multimodal/multimodal_notebooks/multimodal-12-kosmos-2.ipynb,TODO,planned
multimodal,google/pix2struct-base,Pix2Struct Base,UI screenshot to text,transformers,CPU/GPU,8–16GB,Prepare screenshot datasets; MIT,multimodal/multimodal_notebooks/multimodal-13-pix2struct-base.ipynb,TODO,planned
multimodal,Salesforce/blip2-flan-t5-xl,BLIP2 FLAN-T5 XL,Vision-language generation,transformers,CPU/GPU,16–32GB,Heavy; run fp16; MIT,multimodal/multimodal_notebooks/multimodal-14-blip2-flan-t5-xl.ipynb,TODO,planned
multimodal,HuggingFaceM4/idefics2-8b-instruct,Idefics2 8B,Large VLM instruction,transformers,CPU/GPU,32GB+,Needs multi-GPU or 4-bit; license HF,multimodal/multimodal_notebooks/multimodal-15-idefics2-8b.ipynb,TODO,planned
multimodal,adept/fuyu-8b,Fuyu 8B,OCR aware multimodal chat,transformers,CPU/GPU,32GB+,Check Adept license; needs kv cache,multimodal/multimodal_notebooks/multimodal-16-fuyu-8b.ipynb,TODO,planned
multimodal,stabilityai/stable-diffusion-2-1-base,Stable Diffusion 2.1,Text to image generation,"diffusers,torchvision",CPU/GPU,16–32GB,GPU strongly advised; CreativeML license,multimodal/multimodal_notebooks/multimodal-17-stable-diffusion-2-1.ipynb,TODO,planned
multimodal,stabilityai/sdxl-turbo,SDXL Turbo,Real-time diffusion generation,diffusers,CPU/GPU,32GB+,Requires GPU; note non-commercial,multimodal/multimodal_notebooks/multimodal-18-sdxl-turbo.ipynb,TODO,planned
multimodal,microsoft/layoutlmv3-base,LayoutLMv3 Base,Document question answering,"transformers,datasets",CPU/GPU,8–16GB,Needs layoutlm processor; MIT,multimodal/multimodal_notebooks/multimodal-19-layoutlmv3-base.ipynb,TODO,planned
multimodal,facebook/imagebind-1.2b,ImageBind 1.2B,Unified audio image text embeddings,transformers,CPU/GPU,16–32GB,Heavy; requires multi modality inputs,multimodal/multimodal_notebooks/multimodal-20-imagebind-1-2b.ipynb,TODO,planned
