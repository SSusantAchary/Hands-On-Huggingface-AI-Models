#!/usr/bin/env python3
"""Render the 80-model notebook catalog into README and docs."""
from __future__ import annotations

import csv
import shutil
from collections import defaultdict
from pathlib import Path
from typing import Dict, List

ROOT = Path(__file__).resolve().parents[1]
CATALOG_CSV = ROOT / "meta" / "notebook_catalog.csv"
README = ROOT / "README.md"
CATALOG_OVERVIEW = ROOT / "notebooks" / "catalog.md"

EXPECTED_HEADERS = [
    "section",
    "model_id",
    "model_name",
    "use_case",
    "library_deps",
    "hardware_profile",
    "ram_usage_estimate",
    "notes",
    "notebook_path",
    "colab_link",
    "status",
]
SECTIONS_ORDER = ["audio", "multimodal", "nlp", "vision"]
SECTION_LABELS = {
    "audio": "Audio",
    "multimodal": "Multimodal",
    "nlp": "NLP",
    "vision": "Vision",
}
SECTION_DIRS = {sec: ROOT / "notebooks" / sec for sec in SECTIONS_ORDER}
OLD_DOCS_OVERVIEW = ROOT / "docs" / "catalog.md"
OLD_DOCS_SECTION_DIR = ROOT / "docs" / "catalog"
OLD_DOCS_SECTION_SUBDIRS = [ROOT / "docs" / sec for sec in SECTIONS_ORDER]
MARKER_START = "<!-- CATALOG:START -->"
MARKER_NOTE = "<!-- generated by /scripts/build_catalog.py ; do not edit manually -->"
MARKER_END = "<!-- CATALOG:END -->"
REFERENCE_LINKS: Dict[str, List[tuple[str, str, str, str]]] = {
    "nlp": [
        ("ðŸ–¥ï¸", "Text classification (BERT/DistilBERT, PyTorch)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb"),
        ("ðŸš€", "Text classification (TF/Keras)", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb"),
        ("ðŸ–¥ï¸", "DistilBERT IMDB fine-tuning (HF Course)", "GitHub", "https://github.com/huggingface/course/blob/main/chapter3/classification.ipynb"),
        ("ðŸš€", "RoBERTa sentiment (GLUE SST-2) Trainer", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/pytorch/quicktour.ipynb"),
        ("ðŸ–¥ï¸", "Token classification / NER (BERT)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/token_classification.ipynb"),
        ("ðŸ–¥ï¸", "Question answering (SQuAD, BERT)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb"),
        ("ðŸ–¥ï¸", "Summarization (T5/BART)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb"),
        ("ðŸ–¥ï¸", "Translation (Marian / Helsinki-NLP)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/translation.ipynb"),
        ("ðŸš€", "Instruction-tuned FLAN-T5 generation", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_generation.ipynb"),
        ("ðŸš€", "LoRA/QLoRA with PEFT (sequence classification)", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/peft/peft_training_text_classification.ipynb"),
        ("ðŸš€", "LoRA/QLoRA for causal-LM (generation)", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/peft/peft_lora_int8_int4.ipynb"),
        ("ðŸ–¥ï¸", "Sentence-Transformers semantic search (all-MiniLM)", "Docs", "https://www.sbert.net/examples/applications/semantic-search/README.html"),
        ("ðŸ–¥ï¸", "SBERT multilingual retrieval", "Docs", "https://www.sbert.net/examples/applications/semantic-search/semantic_search_ml-qa/README.html"),
        ("ðŸ–¥ï¸", "bge-* embeddings quickstart", "Docs", "https://github.com/FlagOpen/FlagEmbedding/blob/master/docs/text_embedding/quick_start_EN.md"),
        ("ðŸ–¥ï¸", "e5 embeddings & retrieval", "GitHub", "https://github.com/intfloat/e5-mistral-7b-instruct#usage"),
        ("ðŸ–¥ï¸", "Reranking with mixedbread-ai / Cross-Encoders", "Docs", "https://www.sbert.net/examples/applications/cross-encoder/README.html"),
        ("ðŸš€", "DistilBERT QA (Trainer + evaluate)", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb"),
        ("ðŸš€", "Pipeline zero-shot classification", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/pipeline_tutorial.ipynb"),
        ("ðŸ–¥ï¸", "Datasets streaming + tokenization (large corpora)", "GitHub", "https://github.com/huggingface/course/blob/main/chapter5/processing.ipynb"),
        ("ðŸ–¥ï¸", "Evaluate metrics (accuracy/F1/ROUGE)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/evaluate.ipynb"),
    ],
    "vision": [
        ("ðŸ–¥ï¸", "ViT image classification (Imagenette)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb"),
        ("ðŸ–¥ï¸", "CLIP zero-shot classification", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/zero_shot_image_classification.ipynb"),
        ("ðŸ–¥ï¸", "CLIP retrieval (textâ†”image)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/image_text_retrieval.ipynb"),
        ("ðŸ–¥ï¸", "DETR object detection (COCO-style)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/object_detection.ipynb"),
        ("ðŸ–¥ï¸", "SegFormer semantic segmentation", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb"),
        ("ðŸš€", "Mask2Former segmentation (HF example)", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/vision_mask2former.ipynb"),
        ("ðŸš€", "SAM â€“ Segment Anything demo", "Colab", "https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb"),
        ("ðŸš€", "GroundingDINO open-vocabulary detection", "Colab", "https://colab.research.google.com/github/IDEA-Research/GroundingDINO/blob/main/demo/GroundingDINO_Demo.ipynb"),
        ("ðŸš€", "TrOCR OCR (printed text)", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/ocr.ipynb"),
        ("ðŸš€", "Donut document understanding (OCR-free)", "Colab", "https://colab.research.google.com/github/clovaai/donut/blob/master/demo.ipynb"),
        ("ðŸš€", "DPT / MiDaS depth estimation", "Colab", "https://colab.research.google.com/github/isl-org/MiDaS/blob/master/notebooks/midas.ipynb"),
        ("ðŸš€", "Depth-Anything HF demo notebook", "Colab", "https://colab.research.google.com/github/LiheYoung/Depth-Anything/blob/main/notebooks/depth_anything_v2_demo.ipynb"),
        ("ðŸš€", "Pose estimation with MMPose + HF datasets", "Colab", "https://colab.research.google.com/github/open-mmlab/mmpose/blob/main/demo/MMPose_Tutorial.ipynb"),
        ("ðŸ–¥ï¸", "Image feature extraction (ViT as encoder)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/image_feature_extraction.ipynb"),
        ("ðŸš€", "BLIP imageâ€“text retrieval", "Colab", "https://colab.research.google.com/github/salesforce/BLIP/blob/main/notebooks/demo.ipynb"),
        ("ðŸš€", "LayoutLMv3 document layout tasks", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/layoutlmv3_document_ai.ipynb"),
        ("ðŸš€", "Owl-ViT open-vocabulary detection", "Colab", "https://colab.research.google.com/github/google-research/scenic/blob/main/scenic/projects/owl_vit/notebooks/OWLv2_demo.ipynb"),
        ("ðŸš€", "Florence-2 zero-shot vision tasks", "Colab", "https://colab.research.google.com/github/microsoft/Florence-2/blob/main/notebooks/florence2_demo.ipynb"),
        ("ðŸš€", "Vision fine-tune with timm + HF", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/vision_timm_finetune.ipynb"),
        ("ðŸ–¥ï¸", "Diffusers image generation quickstart", "GitHub", "https://github.com/huggingface/notebooks/blob/main/diffusers/stable_diffusion/stable_diffusion_intro.ipynb"),
    ],
    "audio": [
        ("ðŸ–¥ï¸", "Whisper tiny/base ASR (HF pipeline)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/automatic_speech_recognition.ipynb"),
        ("ðŸš€", "Whisper fine-tuning (English subset)", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/asr_fine_tuning_whisper.ipynb"),
        ("ðŸ–¥ï¸", "wav2vec2 ASR (base-960h)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/wav2vec2_asr.ipynb"),
        ("ðŸš€", "HuBERT audio classification (SUPERB)", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification_superb.ipynb"),
        ("ðŸš€", "Keyword spotting (Speech Commands)", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/keyword_spotting.ipynb"),
        ("ðŸš€", "Speaker verification (ECAPA-TDNN, SpeechBrain)", "Colab", "https://colab.research.google.com/github/speechbrain/speechbrain/blob/develop/recipes/VoxCeleb/SpeakerRec/SVECAPA.ipynb"),
        ("ðŸš€", "Speaker diarization (pyannote)", "Colab", "https://colab.research.google.com/github/pyannote/pyannote-audio/blob/develop/tutorials/diarization_api.ipynb"),
        ("ðŸš€", "Audio emotion recognition (SUPERB ER)", "Colab", "https://colab.research.google.com/github/superbbenchmark/superb/blob/master/notebook/SUPERB_ER_demo.ipynb"),
        ("ðŸš€", "TTS (Coqui-TTS basic colab)", "Colab", "https://colab.research.google.com/github/coqui-ai/TTS/blob/dev/notebooks/TTS_inference_demo.ipynb"),
        ("ðŸš€", "Voice activity detection (pyannote VAD)", "Colab", "https://colab.research.google.com/github/pyannote/pyannote-audio/blob/develop/tutorials/pipeline_demo.ipynb"),
        ("ðŸš€", "WavLM ASR / embeddings demo", "Colab", "https://colab.research.google.com/github/microsoft/unilm/blob/master/wavlm/notebooks/WavLM_Demo.ipynb"),
        ("ðŸš€", "XLS-R multilingual ASR", "Colab", "https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_XLSR_Wav2Vec2_on_Arabic_ASR_with_Common_Voice.ipynb"),
        ("ðŸš€", "Audio tagging (UrbanSound8K with HF)", "Colab", "https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/audio_classification_hf.ipynb"),
        ("ðŸš€", "Streaming ASR with transformers", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/asr_streaming.ipynb"),
        ("ðŸš€", "Textless NLP (HuBERT units) demo", "Colab", "https://colab.research.google.com/github/facebookresearch/textlesslib/blob/main/notebooks/demo.ipynb"),
        ("ðŸš€", "Music tagging with AST", "Colab", "https://colab.research.google.com/github/qiuqiangkong/audioset_tagging_cnn/blob/master/colab/ast_audioset_demo.ipynb"),
        ("ðŸš€", "Silero VAD + ASR integration", "Colab", "https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples/silero_vad_colab.ipynb"),
        ("ðŸš€", "Audio augmentation & features (librosa)", "Colab", "https://colab.research.google.com/github/musikalkemist/AudioSignalProcessingForML/blob/master/03-Audio-Data-Augmentation.ipynb"),
        ("ðŸš€", "torchaudio pipeline tutorial", "Colab", "https://colab.research.google.com/github/pytorch/tutorials/blob/main/beginner_source/audio_classifier_tutorial.ipynb"),
        ("ðŸš€", "Audio to embeddings (CLAP/LAION)", "Colab", "https://colab.research.google.com/github/LAION-AI/CLAP/blob/main/notebooks/CLAP_demo.ipynb"),
    ],
    "multimodal": [
        ("ðŸš€", "BLIP image captioning (Salesforce)", "Colab", "https://colab.research.google.com/github/salesforce/BLIP/blob/main/notebooks/demo.ipynb"),
        ("ðŸš€", "BLIP-2 (OPT-2.7B) demo", "Colab", "https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/blip2/eval/blip2_eval_demo.ipynb"),
        ("ðŸ–¥ï¸", "CLIP retrieval (textâ†”image)", "GitHub", "https://github.com/huggingface/notebooks/blob/main/examples/image_text_retrieval.ipynb"),
        ("ðŸš€", "OWL-ViT open-vocabulary detection", "Colab", "https://colab.research.google.com/github/google-research/scenic/blob/main/scenic/projects/owl_vit/notebooks/OWLv2_demo.ipynb"),
        ("ðŸš€", "Qwen2-VL small demo notebook", "Colab", "https://colab.research.google.com/github/QwenLM/Qwen2-VL/blob/main/notebooks/Qwen2_VL_Colab_Demo.ipynb"),
        ("ðŸš€", "OpenFlamingo 3B demo", "Colab", "https://colab.research.google.com/github/mlfoundations/open_flamingo/blob/main/notebooks/open_flamingo_vqa_demo.ipynb"),
        ("ðŸš€", "LLaVA 1.5 demo (VLM chat)", "Colab", "https://colab.research.google.com/github/haotian-liu/LLaVA/blob/main/docs/colab/llava_colab.ipynb"),
        ("ðŸš€", "Kosmos-2 grounding demo", "Colab", "https://colab.research.google.com/github/microsoft/unilm/blob/master/kosmos-2/notebooks/Kosmos-2_Demo.ipynb"),
        ("ðŸš€", "Florence-2 multi-task vision-language", "Colab", "https://colab.research.google.com/github/microsoft/Florence-2/blob/main/notebooks/florence2_demo.ipynb"),
        ("ðŸš€", "ALBEF retrieval/captioning (LAVIS)", "Colab", "https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/albef/eval/albef_eval_demo.ipynb"),
        ("ðŸš€", "ImageBind (audio-image-text) demo", "Colab", "https://colab.research.google.com/github/facebookresearch/ImageBind/blob/main/notebooks/ImageBind_Demo.ipynb"),
        ("ðŸš€", "CLIP Interrogator (caption from image)", "Colab", "https://colab.research.google.com/github/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator.ipynb"),
        ("ðŸš€", "Video-Q&A (Video-LLaVA style)", "Colab", "https://colab.research.google.com/github/LanguageBind/Video-LLaVA/blob/main/colab/Video-LLaVA-1.5-7B-colab.ipynb"),
        ("ðŸš€", "Multimodal RAG with CLIP embeddings", "Colab", "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multimodal_retrieval.ipynb"),
        ("ðŸš€", "GroundingDINO + SAM pipeline", "Colab", "https://colab.research.google.com/github/IDEA-Research/Grounded-Segment-Anything/blob/main/demo/Grounded_Segment_Anything.ipynb"),
        ("ðŸš€", "Pix2Struct (screen-to-text) demo", "Colab", "https://colab.research.google.com/github/google-research/pix2struct/blob/main/notebooks/pix2struct_colab_demo.ipynb"),
        ("ðŸš€", "Donut + ViTSTR document VQA", "Colab", "https://colab.research.google.com/github/clovaai/donut/blob/master/demo_docvqa.ipynb"),
        ("ðŸš€", "CLAP multimodal audio-text retrieval", "Colab", "https://colab.research.google.com/github/LAION-AI/CLAP/blob/main/notebooks/CLAP_demo.ipynb"),
        ("ðŸš€", "BLIP-2 VQA eval (LAVIS)", "Colab", "https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/blip2/eval/vqa_eval.ipynb"),
        ("ðŸš€", "MiniGPT-4 style demo (open variant)", "Colab", "https://colab.research.google.com/github/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT-4.ipynb"),
    ],
}


def load_catalog() -> List[Dict[str, str]]:
    if not CATALOG_CSV.exists():
        raise SystemExit(f"Catalog CSV not found: {CATALOG_CSV}")

    with CATALOG_CSV.open("r", encoding="utf-8") as handle:
        reader = csv.DictReader(handle)
        headers = reader.fieldnames
        if headers != EXPECTED_HEADERS:
            raise SystemExit(
                "notebook_catalog.csv headers mismatch.\n"
                f"Expected: {EXPECTED_HEADERS}\n"
                f"Found:    {headers}"
            )
        rows = [row for row in reader]

    if len(rows) < 80:
        raise SystemExit(f"Catalog must contain at least 80 rows; found {len(rows)}")

    by_section = defaultdict(int)
    for row in rows:
        section = row["section"].strip().lower()
        if section not in SECTIONS_ORDER:
            raise SystemExit(f"Unknown section '{row['section']}' in catalog.")
        by_section[section] += 1

    for section in SECTIONS_ORDER:
        if by_section[section] == 0:
            raise SystemExit(f"Section '{section}' has no entries.")

    return rows


def condensed_table(rows: List[Dict[str, str]]) -> str:
    selected: Dict[str, List[Dict[str, str]]] = {sec: [] for sec in SECTIONS_ORDER}
    for row in rows:
        sec = row["section"].strip().lower()
        if sec in selected and len(selected[sec]) < 4:
            selected[sec].append(row)

    for sec in SECTIONS_ORDER:
        if len(selected[sec]) < 4:
            raise SystemExit(f"Section '{sec}' needs at least 4 entries for condensed table.")

    header = "| Notebook | Use case | Deps | Hardware | RAM |\n|---|---|---|---|---|"
    lines = [header]

    for sec in SECTIONS_ORDER:
        for row in selected[sec]:
            model_link = f"https://huggingface.co/{row['model_id']}"
            tag = sec.upper()
            notebook_cell = f"**{tag}** Â· [{row['model_name']}]({model_link})"
            use_case = row["use_case"].strip()
            deps = ", ".join(part.strip() for part in row["library_deps"].split(","))
            hardware = row["hardware_profile"].strip()
            ram = row["ram_usage_estimate"].strip()
            lines.append(f"| {notebook_cell} | {use_case} | {deps} | {hardware} | {ram} |")

    lines.append("")
    lines.append("[View the full 80-model catalog â†’](./notebooks/catalog.md)")
    return "\n".join(lines)


def build_section_pages(rows: List[Dict[str, str]]) -> Dict[str, str]:
    pages: Dict[str, str] = {}
    for sec in SECTIONS_ORDER:
        section_rows = [row for row in rows if row["section"].strip().lower() == sec]
        title = SECTION_LABELS.get(sec, sec.capitalize())
        lines: List[str] = [f"# {title} Notebook Catalog", ""]
        lines.append("| Model | Use case | Deps | Hardware | RAM | Notes | Notebook |")
        lines.append("|---|---|---|---|---|---|---|")
        for row in section_rows:
            model_link = f"https://huggingface.co/{row['model_id']}"
            model_cell = (
                f"[{row['model_name']}]({model_link})"
                f"<br><sub>{row['model_id']}</sub>"
            )
            use_case = row["use_case"].strip()
            deps = ", ".join(part.strip() for part in row["library_deps"].split(","))
            hardware = row["hardware_profile"].strip()
            ram = row["ram_usage_estimate"].strip()
            notes = row["notes"].strip() or "â€”"
            notebook_path = row["notebook_path"].strip()
            notebook_cell = notebook_path if notebook_path and notebook_path != "TODO" else "â€”"
            lines.append(
                f"| {model_cell} | {use_case} | {deps} | {hardware} | {ram} | {notes} | {notebook_cell} |"
            )
        lines.append("")
        lines.append("_Source of truth: `/meta/notebook_catalog.csv`._")
        lines.append("")
        references = REFERENCE_LINKS.get(sec, [])
        if references:
            lines.append("## Reference notebooks")
            lines.append("")
            lines.append("| | Notebook | Link |")
            lines.append("|---|---|---|")
            for emoji, title, label, url in references:
                lines.append(f"| {emoji} | {title} | [{label}]({url}) |")
            lines.append("")
        pages[sec] = "\n".join(lines)
    return pages


def update_readme(table_markdown: str) -> None:
    text = README.read_text(encoding="utf-8")
    start = text.find(MARKER_START)
    end = text.find(MARKER_END)
    if start == -1 or end == -1:
        raise SystemExit("Catalog markers not found in README.md")
    end += len(MARKER_END)
    block = f"{MARKER_START}\n{MARKER_NOTE}\n{table_markdown}\n{MARKER_END}"
    updated = text[:start] + block + text[end:]
    README.write_text(updated, encoding="utf-8")


def main() -> None:
    rows = load_catalog()
    table_md = condensed_table(rows)
    section_pages = build_section_pages(rows)
    update_readme(table_md)
    if OLD_DOCS_SECTION_DIR.exists():
        shutil.rmtree(OLD_DOCS_SECTION_DIR)
    for old_dir in OLD_DOCS_SECTION_SUBDIRS:
        if old_dir.exists():
            shutil.rmtree(old_dir)
    if OLD_DOCS_OVERVIEW.exists():
        OLD_DOCS_OVERVIEW.unlink()

    index_lines = [
        "# Notebook Catalog Overview",
        "",
        "Browse per-domain model picks ready for notebook implementation:",
        "",
    ]
    for sec in SECTIONS_ORDER:
        index_lines.append(f"- [{SECTION_LABELS.get(sec, sec.capitalize())}](./{sec}/{sec}_catalog.md)")
    index_lines.extend(
        [
            "",
            "_Source of truth: `/meta/notebook_catalog.csv`._",
            "",
        ]
    )
    CATALOG_OVERVIEW.parent.mkdir(parents=True, exist_ok=True)
    CATALOG_OVERVIEW.write_text("\n".join(index_lines), encoding="utf-8")

    for sec, content in section_pages.items():
        target_dir = SECTION_DIRS[sec]
        target_dir.mkdir(parents=True, exist_ok=True)
        target = target_dir / f"{sec}_catalog.md"
        target.write_text(content, encoding="utf-8")

    print("Catalog build complete.")


if __name__ == "__main__":
    main()
