# Core Computer Vision Models Overview
## Classification Models

| **Model**                    | **Task**             | **Code / Weights**                                                                                         | **Paper**                                                                | **License**             |
| ---------------------------- | -------------------- | ---------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | ----------------------- |
| **AlexNet**                  | Image Classification | [Cuda-ConvNet2 (Alex Krizhevsky)](https://github.com/akrizhevsky/cuda-convnet2)                            | *ImageNet Classification with Deep CNNs* (NeurIPS 2012)                  | Apache 2.0              |
| **VGG** (VGG16)              | Image Classification | [Oxford VGG Model Release](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)                           | *Very Deep Conv. Networks for Large-Scale Image Recognition* (ICLR 2015) | Creative Commons BY 4.0 |
| **ResNet**                   | Image Classification | [Kaiming He’s ResNet Repo](https://github.com/KaimingHe/deep-residual-networks)                            | *Deep Residual Learning for Image Recognition* (CVPR 2016)               | MIT                     |
| **DenseNet**                 | Image Classification | [DenseNet Official (Torch)](https://github.com/liuzhuang13/DenseNet)                                       | *Densely Connected Convolutional Networks* (CVPR 2017)                   | BSD 3-Clause            |
| **EfficientNet**             | Image Classification | [TensorFlow TPU EfficientNet](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet)  | *EfficientNet: Rethinking Model Scaling* (ICML 2019)                     | Apache 2.0              |
| **Vision Transformer (ViT)** | Image Classification | [Google Research ViT](https://github.com/google-research/vision_transformer)                               | *An Image is Worth 16x16 Words* (ICLR 2021)                              | Apache 2.0              |
| **Swin Transformer**         | Image Classification | [Microsoft Swin-Transformer](https://github.com/microsoft/Swin-Transformer)                                | *Swin Transformer: Hierarchical Vision Transformer* (ICCV 2021)          | MIT                     |
| **ConvNeXt**                 | Image Classification | [Facebook ConvNeXt](https://github.com/facebookresearch/ConvNeXt)                                          | *A ConvNet for the 2020s* (CVPR 2022)                                    | MIT                     |

# Object Detection Models

| **Model**                      | **Task**                   | **Code / Weights**                                                     | **Paper**                                                            | **License**           |
| ------------------------------ | -------------------------- | ---------------------------------------------------------------------- | -------------------------------------------------------------------- | --------------------- |
| **R-CNN**                      | Object Detection           | [Ross Girshick’s R-CNN (Caffe)](https://github.com/rbgirshick/rcnn)    | *Rich Feature Hierarchies for Accurate Object Detection* (CVPR 2014) | BSD 2-Clause          |
| **Fast R-CNN**                 | Object Detection           | [Fast R-CNN (Caffe)](https://github.com/rbgirshick/fast-rcnn)          | *Fast R-CNN* (ICCV 2015)                                             | MIT                   |
| **Faster R-CNN**               | Object Detection           | [Faster R-CNN (Python)](https://github.com/rbgirshick/py-faster-rcnn)  | *Faster R-CNN: Towards Real-Time Object Detection* (NIPS 2015)       | MIT                   |
| **YOLO (v1)**                  | Object Detection           | [Darknet (PJ Reddie)](https://github.com/pjreddie/darknet)             | *You Only Look Once: Unified Real-Time Detection* (CVPR 2016)        | YOLO License (custom) |
| **SSD** (Single Shot Detector) | Object Detection           | [Caffe SSD (Wei Liu)](https://github.com/weiliu89/caffe/tree/ssd)      | *SSD: Single Shot MultiBox Detector* (ECCV 2016)                     | BSD 2-Clause (Caffe)  |
| **RetinaNet**                  | Object Detection           | [Keras RetinaNet (Fizyr)](https://github.com/fizyr/keras-retinanet)    | *Focal Loss for Dense Object Detection* (ICCV 2017)                  | Apache 2.0            |
| **DETR**                       | Object Detection           | [Facebook DETR (PyTorch)](https://github.com/facebookresearch/detr)    | *End-to-End Object Detection with Transformers* (ECCV 2020)          | Apache 2.0            |
| **Mask R-CNN**                 | Object Det. + Segmentation | [Mask R-CNN (Matterport)](https://github.com/matterport/Mask_RCNN)     | *Mask R-CNN* (ICCV 2017)                                             | MIT                   |

# Segmentation Models

| **Model**                  | **Task**                   | **Code / Weights**                                                                        | **Paper**                                                                   | **License**             |
| -------------------------- | -------------------------- | ----------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- | ----------------------- |
| **FCN** (Fully Conv Net)   | Semantic Segmentation      | [FCN Reference (Caffe)](https://github.com/shelhamer/fcn.berkeleyvision.org)              | *Fully Conv. Networks for Semantic Segmentation* (CVPR 2015)                | BSD 2-Clause            |
| **U-Net**                  | Medical Image Segmentation | [U-Net Keras Implementation](https://github.com/zhixuhao/unet)                            | *U-Net: Conv Nets for Biomedical Segmentation* (MICCAI 2015)                | MIT                     |
| **DeepLab**                | Semantic Segmentation      | [DeepLab (TF Models)](https://github.com/tensorflow/models/tree/master/research/deeplab)  | *DeepLab: Semantic Image Segmentation with Atrous Convolution* (TPAMI 2017) | Apache 2.0              |
| **PSPNet**                 | Semantic Segmentation      | [PSPNet Official (Caffe)](https://github.com/hszhao/PSPNet)                               | *Pyramid Scene Parsing Network* (CVPR 2017)                                 | BSD 2-Clause (Caffe)    |
| **SegFormer**              | Semantic Segmentation      | [NVIDIA SegFormer (PyTorch)](https://github.com/NVlabs/SegFormer)                         | *SegFormer: Simple and Efficient Design for Segmentation* (NeurIPS 2021)    | NVIDIA Source Code (NC) |
| **SAM** (Segment Anything) | Promptable Segmentation    | [Meta Segment-Anything](https://github.com/facebookresearch/segment-anything)             | *Segment Anything* (CVPR 2023)                                              | Apache 2.0              |

# 3D & Video Models

| Model Name                | Model Usage                                              | Model Code/Weights                                                                                                                                                           | Model Paper                                                          | Model License           |
| ------------------------- | -------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------- | ----------------------- |
| **PointNet++**            | 3D point-cloud classification/segmentation               | [https://github.com/charlesq34/pointnet2](https://github.com/charlesq34/pointnet2)                                                                                           | [https://arxiv.org/abs/1706.02413](https://arxiv.org/abs/1706.02413) | MIT                     |
| **DGCNN**                 | 3D point-cloud classification/segmentation (graph-based) | [https://github.com/WangYueFt/dgcnn](https://github.com/WangYueFt/dgcnn)                                                                                                     | [https://arxiv.org/abs/1801.07829](https://arxiv.org/abs/1801.07829) | MIT                     |
| **KPConv**                | Point-cloud conv. with kernel points                     | [https://github.com/HuguesTHOMAS/KPConv](https://github.com/HuguesTHOMAS/KPConv)                                                                                             | [https://arxiv.org/abs/1904.08889](https://arxiv.org/abs/1904.08889) | MIT                     |
| **MinkowskiEngine**       | Sparse 3D(–4D) CNN backbone/library                      | [https://github.com/NVIDIA/MinkowskiEngine](https://github.com/NVIDIA/MinkowskiEngine)                                                                                       | [https://arxiv.org/abs/1904.08755](https://arxiv.org/abs/1904.08755) | MIT                     |
| **SparseConvNet**         | Submanifold sparse 3D CNNs                               | [https://github.com/facebookresearch/SparseConvNet](https://github.com/facebookresearch/SparseConvNet)                                                                       | [https://arxiv.org/abs/1706.01307](https://arxiv.org/abs/1706.01307) | BSD-3-Clause            |
| **Point Transformer**     | Transformer for point clouds                             | [https://github.com/POSTECH-CVLab/point-transformer](https://github.com/POSTECH-CVLab/point-transformer)                                                                     | [https://arxiv.org/abs/2012.09164](https://arxiv.org/abs/2012.09164) | MIT                     |
| **PointNeXt**             | Strong point-based 3D backbone                           | [https://github.com/guochengqian/PointNeXt](https://github.com/guochengqian/PointNeXt)                                                                                       | [https://arxiv.org/abs/2206.04670](https://arxiv.org/abs/2206.04670) | MIT                     |
| **Point-BERT**            | Self-supervised pretraining for 3D                       | [https://github.com/lulutang0608/Point-BERT](https://github.com/lulutang0608/Point-BERT)                                                                                     | [https://arxiv.org/abs/2111.14819](https://arxiv.org/abs/2111.14819) | MIT                     |
| **PointRCNN**             | 3D object detection from raw point clouds                | [https://github.com/sshaoshuai/PointRCNN](https://github.com/sshaoshuai/PointRCNN)                                                                                           | [https://arxiv.org/abs/1812.04244](https://arxiv.org/abs/1812.04244) | MIT                     |
| **SECOND**                | Voxel-based 3D object detection                          | [https://github.com/traveller59/second.pytorch](https://github.com/traveller59/second.pytorch)                                                                               | [https://arxiv.org/abs/1712.09430](https://arxiv.org/abs/1712.09430) | MIT                     |
| **PointPillars**          | Real-time LiDAR 3D detection                             | [https://github.com/open-mmlab/OpenPCDet](https://github.com/open-mmlab/OpenPCDet)                                                                                           | [https://arxiv.org/abs/1812.05784](https://arxiv.org/abs/1812.05784) | Apache-2.0              |
| **PV-RCNN**               | Point–voxel fusion 3D detection                          | [https://github.com/open-mmlab/OpenPCDet](https://github.com/open-mmlab/OpenPCDet)                                                                                           | [https://arxiv.org/abs/1912.13192](https://arxiv.org/abs/1912.13192) | Apache-2.0              |
| **CenterPoint**           | Center-based 3D detection & tracking                     | [https://github.com/tianweiy/CenterPoint](https://github.com/tianweiy/CenterPoint)                                                                                           | [https://arxiv.org/abs/2006.11275](https://arxiv.org/abs/2006.11275) | MIT                     |
| **Nerfstudio**            | NeRF training/inference toolbox                          | [https://github.com/nerfstudio-project/nerfstudio](https://github.com/nerfstudio-project/nerfstudio)                                                                         | [https://arxiv.org/abs/2302.04264](https://arxiv.org/abs/2302.04264) | Apache-2.0              |
| **instant-ngp (NeRF)**    | Fast NeRF with hash encodings                            | [https://github.com/NVlabs/instant-ngp](https://github.com/NVlabs/instant-ngp)                                                                                               | [https://arxiv.org/abs/2201.05989](https://arxiv.org/abs/2201.05989) | NVIDIA Source Code (NC) |
| **mip-NeRF**              | Anti-aliased NeRF rendering                              | [https://github.com/google/mipnerf](https://github.com/google/mipnerf)                                                                                                       | [https://arxiv.org/abs/2103.13415](https://arxiv.org/abs/2103.13415) | Apache-2.0              |
| **mip-NeRF 360**          | Unbounded scenes NeRF                                    | [https://github.com/google-research/mipnerf360](https://github.com/google-research/mipnerf360)                                                                               | [https://arxiv.org/abs/2111.12077](https://arxiv.org/abs/2111.12077) | Apache-2.0              |
| **TensoRF**               | Tensor factorization radiance fields                     | [https://github.com/apchenstu/TensoRF](https://github.com/apchenstu/TensoRF)                                                                                                 | [https://arxiv.org/abs/2203.09517](https://arxiv.org/abs/2203.09517) | MIT                     |
| **PlenOctrees**           | Real-time NeRF via octrees                               | [https://github.com/sxyu/plenoctree](https://github.com/sxyu/plenoctree)                                                                                                     | [https://arxiv.org/abs/2103.14024](https://arxiv.org/abs/2103.14024) | MIT                     |
| **3D Gaussian Splatting** | Fast radiance field rendering                            | [https://github.com/graphdeco-inria/gaussian-splatting](https://github.com/graphdeco-inria/gaussian-splatting)                                                               | [https://arxiv.org/abs/2308.04079](https://arxiv.org/abs/2308.04079) | GPL-3.0                 |
| **I3D**                   | Inflated 3D ConvNet for video cls.                       | [https://github.com/deepmind/kinetics-i3d](https://github.com/deepmind/kinetics-i3d)                                                                                         | [https://arxiv.org/abs/1705.07750](https://arxiv.org/abs/1705.07750) | Apache-2.0              |
| **SlowFast**              | Dual-pathway video recognition                           | [https://github.com/facebookresearch/SlowFast](https://github.com/facebookresearch/SlowFast)                                                                                 | [https://arxiv.org/abs/1812.03982](https://arxiv.org/abs/1812.03982) | Apache-2.0              |
| **X3D**                   | Efficient video nets (scaling)                           | [https://github.com/facebookresearch/SlowFast](https://github.com/facebookresearch/SlowFast)                                                                                 | [https://arxiv.org/abs/2004.04730](https://arxiv.org/abs/2004.04730) | Apache-2.0              |
| **R(2+1)D**               | Factorized 3D convs for video                            | [https://github.com/facebookresearch/VMZ](https://github.com/facebookresearch/VMZ)                                                                                           | [https://arxiv.org/abs/1711.11248](https://arxiv.org/abs/1711.11248) | BSD-3-Clause            |
| **MoViNet**               | Mobile video classification                              | [https://github.com/tensorflow/models/tree/master/official/vision/beta/models/movinet](https://github.com/tensorflow/models/tree/master/official/vision/beta/models/movinet) | [https://arxiv.org/abs/2103.11511](https://arxiv.org/abs/2103.11511) | Apache-2.0              |
| **TimeSformer**           | Space–time ViT for video                                 | [https://github.com/facebookresearch/TimeSformer](https://github.com/facebookresearch/TimeSformer)                                                                           | [https://arxiv.org/abs/2102.05095](https://arxiv.org/abs/2102.05095) | Apache-2.0              |
| **ViViT**                 | Video Vision Transformer                                 | [https://github.com/google-research/scenic/tree/main/scenic/projects/vivit](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit)                       | [https://arxiv.org/abs/2103.15691](https://arxiv.org/abs/2103.15691) | Apache-2.0              |
| **VideoMAE**              | Masked autoencoders for video                            | [https://github.com/MCG-NJU/VideoMAE](https://github.com/MCG-NJU/VideoMAE)                                                                                                   | [https://arxiv.org/abs/2203.12602](https://arxiv.org/abs/2203.12602) | Apache-2.0              |
| **MViT**                  | Multiscale ViT for video                                 | [https://github.com/facebookresearch/SlowFast](https://github.com/facebookresearch/SlowFast)                                                                                 | [https://arxiv.org/abs/2104.11227](https://arxiv.org/abs/2104.11227) | Apache-2.0              |
| **RAFT**                  | Optical flow (video motion)                              | [https://github.com/princeton-vl/RAFT](https://github.com/princeton-vl/RAFT)                                                                                                 | [https://arxiv.org/abs/2003.12039](https://arxiv.org/abs/2003.12039) | BSD-3-Clause            |
| **PWC-Net**               | Optical flow (pyramid, warp, cost)                       | [https://github.com/NVlabs/PWC-Net](https://github.com/NVlabs/PWC-Net)                                                                                                       | [https://arxiv.org/abs/1709.02371](https://arxiv.org/abs/1709.02371) | NVIDIA Source Code (NC) |
| **FlowNet2**              | Optical flow (stacked nets)                              | [https://github.com/NVIDIA/flownet2-pytorch](https://github.com/NVIDIA/flownet2-pytorch)                                                                                     | [https://arxiv.org/abs/1612.01925](https://arxiv.org/abs/1612.01925) | Apache-2.0              |
