<div align="center">

<p><strong>Developer-first, CPU-friendly Hugging Face notebooks with transparent measurements.</strong></p>

<p>
  <a href="/docs/gallery.md" style="padding:12px 20px;margin:4px;display:inline-block;border-radius:8px;background:#1f6feb;color:#fff;text-decoration:none;font-weight:600;">Notebook Gallery</a>
  <a href="/docs/fixes-and-tips.md" style="padding:12px 20px;margin:4px;display:inline-block;border-radius:8px;background:#0d1117;color:#fff;text-decoration:none;font-weight:600;">Fixes &amp; Tips</a>
  <a href="/docs/benchmarks.md" style="padding:12px 20px;margin:4px;display:inline-block;border-radius:8px;background:#6f42c1;color:#fff;text-decoration:none;font-weight:600;">Benchmark Highlights</a>
</p>

[![GitHub Repo stars](https://img.shields.io/github/stars/SSusantAchary/Hands-On-Huggingface-AI-Models?style=social)](https://github.com/SSusantAchary/Hands-On-Huggingface-AI-Models)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Built with ü§ó Transformers](https://img.shields.io/badge/Built%20with-%F0%9F%A4%97%20Transformers-ff4a9c.svg)](https://huggingface.co/transformers)

</div>

---

### Quick Links
- [Run Anywhere Pledge](#run-anywhere-pledge)
- [üß© Hugging Face Ecosystem ‚Äî Dev Quick Picks](#-hugging-face-ecosystem--dev-quick-picks)
- [What‚Äôs Inside](#whats-inside)
- [Quick Start](#quick-start)
- [üßæ Vision OCR Guide](notebooks/vision/OCR_models.md)
- [üé® Vision Text-to-Image Guide](notebooks/vision/text2image_models.md)

## üß© Hugging Face Ecosystem ‚Äî Dev Quick Picks
<sub>Last reviewed: 2025-10-08</sub>
<br>
<sup>Opinionated, regularly used pieces that pair well with ü§ó. Links go to official docs where possible.</sup>

<!-- ECOSYSTEM:START -->
<!-- generated by /scripts/build_ecosystem.py ; do not edit manually -->
| Category | Library | What it‚Äôs for | Where it fits |
|---|---|---|---|
| Core | [Transformers](https://huggingface.co/docs/transformers/index) | SOTA models across text/vision/audio/multimodal; training & inference | Model APIs, Trainer, generate() |
|  | [Datasets](https://huggingface.co/docs/datasets) | Stream/load/share datasets at scale | Data I/O, preprocessing, splits |
|  | [Tokenizers](https://huggingface.co/docs/tokenizers/index) | Fast Rust-backed tokenizers | Prod tokenization, custom vocab |
|  | [Evaluate](https://huggingface.co/docs/evaluate/index) | Metrics & eval pipelines | Reproducible metrics |
|  | [Diffusers](https://huggingface.co/docs/diffusers/index) | Diffusion models for images/video/audio | GenAI imaging/video |
| Training & Post-Training | [Accelerate](https://huggingface.co/docs/accelerate/index) | Device/distributed orchestration | Multi-GPU/TPU/MPS launch |
|  | [PEFT](https://huggingface.co/docs/peft/index) | LoRA/QLoRA & adapter training | Parameter-efficient finetuning |
|  | [TRL](https://huggingface.co/docs/trl/index) | SFT, DPO, PPO, RM for LLMs | Post-training & alignment |
|  | [DeepSpeed](https://www.deepspeed.ai/) | ZeRO + memory-efficient training | Large-model training via Trainer |
|  | [Optimum](https://huggingface.co/docs/optimum/index) | Hardware-specific speedups | ONNX, Intel, NVIDIA, AWS Neuron |
| Quant & Memory | [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) | 8-bit / 4-bit loading & QLoRA | Low-VRAM inference/finetune |
| Serving & Deployment | [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) | High-perf LLM serving | Prod text-gen endpoints |
|  | [Text Embeddings Inference (TEI)](https://github.com/huggingface/text-embeddings-inference) | High-perf embedding serving | Retrieval/semantic search |
|  | [huggingface_hub](https://huggingface.co/docs/huggingface_hub) | Programmatic Hub client | Push/pull models, datasets, Spaces |
| Hub, DataFrames & ETL | [Hub integrations (Polars/Pandas/DuckDB/Dask/Spark/Daft)](https://huggingface.co/docs/hub/en/extensions) | Write/read to Hub repos from tables | Dataset pipelines & exports |
| Apps & Demos | [Spaces + Gradio](https://huggingface.co/docs/hub/en/spaces-sdks-gradio) | Share interactive demos on the Hub | UI for notebooks & models |
<!-- ECOSYSTEM:END -->

How to suggest a library: [open an issue](https://github.com/SSusantAchary/Hands-On-Huggingface-AI-Models/issues/new?template=01-suggest-ecosystem-library.md).

**This week** (see `meta/CHANGELOG.md` for details):
- Gallery kickoff with CPU-first sentiment, ViT Imagenette, and Whisper notebooks.
- Fix of the Week: Metal backend fallback detects missing MPS and guides to CPU.
- Mini-benchmark of the Week: CLIP batch-size sweep (tokens/s placeholder until run).

---

### Run Anywhere Pledge
- CPU is the default path; every notebook runs end-to-end without a GPU.
- Optional toggles enable Apple Silicon (Metal) or CUDA acceleration when present.
- Measurements list RAM/VRAM footprints, first-token latency, and throughput per run.

### What's Inside
- Curated notebook gallery across NLP, vision, audio, multimodal, and serving tasks.
- Single-source benchmark matrix (`benchmarks/matrix.csv`) + lightweight charts.
- Fixes & Tips index (symptom ‚Üí fix ‚Üí verify ‚Üí scope) for common Hugging Face hurdles.
- MkDocs-powered docs site for quick navigation and sharing.

### Quick Start
```bash
git clone https://github.com/SSusantAchary/Hands-On-Huggingface-AI-Models.git
cd Hands-On-Huggingface-AI-Models
python -m venv .venv && source .venv/bin/activate
pip install -r notebooks/requirements-minimal.txt
```
Open any notebook from `/notebooks` in Jupyter, VS Code, or Colab (badges inside each file).

## üîç Vision OCR Highlights
- üßæ **Trending models dashboard** ‚Üí Dive into [Vision OCR Guide](notebooks/vision/OCR_models.md) for the latest `ocr` search snapshot, curated prompts, and resource links.
- üìí **Nanonets-OCR2-3B** ‚Üí Run the official [image2md cookbook](https://github.com/NanoNets/Nanonets-OCR2/blob/main/Nanonets-OCR2-Cookbook/image2md.ipynb) to turn PDFs into structured Markdown.
- ü™Ñ **dots.ocr** ‚Üí Launch the [Colab remote server notebook](https://github.com/rednote-hilab/dots.ocr/blob/master/demo/demo_colab_remote_server.ipynb) for layout-aware parsing with table/formula support.
- üåè **Typhoon-OCR-7B** ‚Üí Use the bilingual [Colab quick start](https://colab.research.google.com/drive/1z4Fm2BZnKcFIoWuyxzzIIIn8oI2GKl3r?usp=sharing) to process Thai/English documents.
- üìö **Manga OCR base** ‚Üí Follow the CLI [usage guide](https://github.com/kha-white/manga-ocr/blob/master/README.md#usage) for vertical, furigana-rich Japanese text.

## üé® Vision Text-to-Image Highlights
- ‚ö° **SD Turbo** ‚Üí See the [Diffusers quickstart](https://huggingface.co/docs/diffusers/using-diffusers/sd_turbo) for 2‚Äì4 step 512¬≤ renders; works on 6GB GPUs or 8GB unified VRAM (M-series).
- üñºÔ∏è **SDXL Base + Refiner** ‚Üí Use the [SDXL text-to-image Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers_doc/en/pytorch/sdxl.ipynb) for photoreal 1024¬≤ generations (‚â•12GB GPU / 16GB unified).
- üåÄ **FLUX.1 family** ‚Üí Try the [FLUX CFG Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/flux_with_cfg.ipynb) for cinematic looks; expect 16‚Äì24GB VRAM or 24GB+ unified memory.
- üåê **Qwen-Image Lightning** ‚Üí Multilingual prompts via the [Qwen2-Image Space](https://huggingface.co/spaces/Qwen/Qwen2-Image); plan on ‚â•16GB VRAM or 20GB unified.
- üî° **Stable Diffusion 3 Medium** ‚Üí Follow the [SD3 pipeline docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion_3) for better text rendering (‚â•18GB GPU / 32GB unified).

## üìí Notebook Catalog (80 models)
Developer-ready shortlist of Hugging Face models we plan to ship as notebooks. Full catalog refreshes from `/meta/notebook_catalog.csv`.

<!-- CATALOG:START -->
<!-- generated by /scripts/build_catalog.py ; do not edit manually -->
| Notebook | Use case | Deps | Hardware | RAM |
|---|---|---|---|---|
| **AUDIO** ¬∑ [Whisper Tiny](https://huggingface.co/openai/whisper-tiny) | ASR on short clips (EN/multilingual) | transformers, torchaudio, soundfile | CPU/GPU/MLX | <4GB |
| **AUDIO** ¬∑ [Wav2Vec2 Base 960h](https://huggingface.co/facebook/wav2vec2-base-960h) | ASR baseline (LibriSpeech-style) | transformers, torchaudio | CPU/GPU/MLX | 4‚Äì8GB |
| **AUDIO** ¬∑ [Whisper Base](https://huggingface.co/openai/whisper-base) | Balanced ASR quality vs speed | transformers, torchaudio, soundfile | CPU/GPU/MLX | 4‚Äì8GB |
| **AUDIO** ¬∑ [Whisper Small](https://huggingface.co/openai/whisper-small) | Improved ASR accuracy | transformers, torchaudio, soundfile | CPU/GPU/MLX | 8‚Äì16GB |
| **MULTIMODAL** ¬∑ [CLIP ViT-B/32](https://huggingface.co/openai/clip-vit-base-patch32) | Zero-shot image classification & retrieval | transformers | CPU/GPU/MLX | 4‚Äì8GB |
| **MULTIMODAL** ¬∑ [BLIP Captioning Base](https://huggingface.co/Salesforce/blip-image-captioning-base) | Image captioning on Flickr8k subset | transformers, datasets | CPU/GPU | 8‚Äì16GB |
| **MULTIMODAL** ¬∑ [BLIP2 OPT 2.7B](https://huggingface.co/Salesforce/blip2-opt-2.7b) | Image captioning with OPT decoder | transformers | CPU/GPU | 16‚Äì32GB |
| **MULTIMODAL** ¬∑ [CLIP ViT-L/14](https://huggingface.co/laion/CLIP-ViT-L-14) | High quality cross-modal retrieval | transformers | CPU/GPU | 8‚Äì16GB |
| **NLP** ¬∑ [BERT base uncased](https://huggingface.co/bert-base-uncased) | Sentiment/IMDB baseline (CPU-first pipeline + LoRA TODO) | transformers, datasets, evaluate | CPU/GPU/MLX | <4GB |
| **NLP** ¬∑ [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | Semantic search embeddings (vector DB ready) | sentence-transformers, transformers | CPU/GPU/MLX | <4GB |
| **NLP** ¬∑ [RoBERTa base](https://huggingface.co/roberta-base) | Sentiment on social reviews baseline | transformers, datasets, evaluate | CPU/GPU/MLX | 4‚Äì8GB |
| **NLP** ¬∑ [DistilBERT base uncased](https://huggingface.co/distilbert-base-uncased) | Real-time sentiment microservice | transformers, datasets | CPU/GPU/MLX | <4GB |
| **VISION** ¬∑ [ViT Base 224](https://huggingface.co/google/vit-base-patch16-224) | Image classification baseline (Imagenette) | transformers, datasets, timm | CPU/GPU/MLX | 4‚Äì8GB |
| **VISION** ¬∑ [DETR ResNet-50](https://huggingface.co/facebook/detr-resnet-50) | Object detection on sample images | transformers, torchvision | CPU/GPU | 8‚Äì16GB |
| **VISION** ¬∑ [ResNet-50](https://huggingface.co/microsoft/resnet-50) | Classic classification transfer | torchvision, timm | CPU/GPU/MLX | 4‚Äì8GB |
| **VISION** ¬∑ [CLIP ViT-B LAION](https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K) | Zero-shot retrieval large corpus | transformers | CPU/GPU | 4‚Äì8GB |

[View the full 80-model catalog ‚Üí](./notebooks/catalog.md)
<!-- CATALOG:END -->

### Who It's For
- Practitioners who need reproducible, CPU-safe Hugging Face experiments.
- Teams validating Metal or CUDA paths without breaking CPU workflows.
- Contributors adding benchmarks, fixes, or doc polish with low friction.

**Prereqs:** Python ‚â•3.10, git, and `pip`. GPU/Metal optional.

### Fix of the Week
- Metal backend fallback detection with CPU guidance ‚Äì documented in `/fixes-and-tips/metal-backend-fallback.md`.

### Mini-benchmark of the Week
- CLIP retrieval batch-size sweep scaffolding ‚Äì see `/docs/benchmarks.md` and run the notebook to populate metrics.

### Cite & License
```
@misc{hands-on-hf,
  author    = {S. Susant Achary},
  title     = {Hands-On Hugging Face AI Models},
  year      = {2025},
  howpublished = {\url{https://github.com/SSusantAchary/Hands-On-Huggingface-AI-Models}}
}
```
Licensed under the [MIT License](LICENSE).

---

Questions or ideas? Open an issue with labels `notebook`, `fix`, `benchmark`, or `apple-silicon` to help us triage fast.
