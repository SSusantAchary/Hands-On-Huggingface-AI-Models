<div align="center">

<p><strong>Developer-first, CPU-friendly Hugging Face notebooks with transparent measurements.</strong></p>

<p>
  <a href="/docs/gallery.md" style="padding:12px 20px;margin:4px;display:inline-block;border-radius:8px;background:#1f6feb;color:#fff;text-decoration:none;font-weight:600;">Notebook Gallery</a>
  <a href="/docs/fixes-and-tips.md" style="padding:12px 20px;margin:4px;display:inline-block;border-radius:8px;background:#0d1117;color:#fff;text-decoration:none;font-weight:600;">Fixes &amp; Tips</a>
  <a href="/docs/benchmarks.md" style="padding:12px 20px;margin:4px;display:inline-block;border-radius:8px;background:#6f42c1;color:#fff;text-decoration:none;font-weight:600;">Benchmark Highlights</a>
</p>

[![GitHub Repo stars](https://img.shields.io/github/stars/SSusantAchary/Hands-On-Huggingface-AI-Models?style=social)](https://github.com/SSusantAchary/Hands-On-Huggingface-AI-Models)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Built with ðŸ¤— Transformers](https://img.shields.io/badge/Built%20with-%F0%9F%A4%97%20Transformers-ff4a9c.svg)](https://huggingface.co/transformers)

</div>

---

### Quick Links
- [Run Anywhere Pledge](#run-anywhere-pledge)
- [ðŸ§© Hugging Face Ecosystem â€” Dev Quick Picks](#-hugging-face-ecosystem--dev-quick-picks)
- [Whatâ€™s Inside](#whats-inside)
- [Quick Start](#quick-start)

## ðŸ§© Hugging Face Ecosystem â€” Dev Quick Picks
<sub>Last reviewed: 2025-10-08</sub>
<br>
<sup>Opinionated, regularly used pieces that pair well with ðŸ¤—. Links go to official docs where possible.</sup>

<!-- ECOSYSTEM:START -->
<!-- generated by /scripts/build_ecosystem.py ; do not edit manually -->
| Category | Library | What itâ€™s for | Where it fits |
|---|---|---|---|
| Core | [Transformers](https://huggingface.co/docs/transformers/index) | SOTA models across text/vision/audio/multimodal; training & inference | Model APIs, Trainer, generate() |
|  | [Datasets](https://huggingface.co/docs/datasets) | Stream/load/share datasets at scale | Data I/O, preprocessing, splits |
|  | [Tokenizers](https://huggingface.co/docs/tokenizers/index) | Fast Rust-backed tokenizers | Prod tokenization, custom vocab |
|  | [Evaluate](https://huggingface.co/docs/evaluate/index) | Metrics & eval pipelines | Reproducible metrics |
|  | [Diffusers](https://huggingface.co/docs/diffusers/index) | Diffusion models for images/video/audio | GenAI imaging/video |
| Training & Post-Training | [Accelerate](https://huggingface.co/docs/accelerate/index) | Device/distributed orchestration | Multi-GPU/TPU/MPS launch |
|  | [PEFT](https://huggingface.co/docs/peft/index) | LoRA/QLoRA & adapter training | Parameter-efficient finetuning |
|  | [TRL](https://huggingface.co/docs/trl/index) | SFT, DPO, PPO, RM for LLMs | Post-training & alignment |
|  | [DeepSpeed](https://www.deepspeed.ai/) | ZeRO + memory-efficient training | Large-model training via Trainer |
|  | [Optimum](https://huggingface.co/docs/optimum/index) | Hardware-specific speedups | ONNX, Intel, NVIDIA, AWS Neuron |
| Quant & Memory | [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) | 8-bit / 4-bit loading & QLoRA | Low-VRAM inference/finetune |
| Serving & Deployment | [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) | High-perf LLM serving | Prod text-gen endpoints |
|  | [Text Embeddings Inference (TEI)](https://github.com/huggingface/text-embeddings-inference) | High-perf embedding serving | Retrieval/semantic search |
|  | [huggingface_hub](https://huggingface.co/docs/huggingface_hub) | Programmatic Hub client | Push/pull models, datasets, Spaces |
| Hub, DataFrames & ETL | [Hub integrations (Polars/Pandas/DuckDB/Dask/Spark/Daft)](https://huggingface.co/docs/hub/en/extensions) | Write/read to Hub repos from tables | Dataset pipelines & exports |
| Apps & Demos | [Spaces + Gradio](https://huggingface.co/docs/hub/en/spaces-sdks-gradio) | Share interactive demos on the Hub | UI for notebooks & models |
<!-- ECOSYSTEM:END -->

How to suggest a library: [open an issue](https://github.com/SSusantAchary/Hands-On-Huggingface-AI-Models/issues/new?template=01-suggest-ecosystem-library.md).

**This week** (see `meta/CHANGELOG.md` for details):
- Gallery kickoff with CPU-first sentiment, ViT Imagenette, and Whisper notebooks.
- Fix of the Week: Metal backend fallback detects missing MPS and guides to CPU.
- Mini-benchmark of the Week: CLIP batch-size sweep (tokens/s placeholder until run).

---

### Run Anywhere Pledge
- CPU is the default path; every notebook runs end-to-end without a GPU.
- Optional toggles enable Apple Silicon (Metal) or CUDA acceleration when present.
- Measurements list RAM/VRAM footprints, first-token latency, and throughput per run.

### What's Inside
- Curated notebook gallery across NLP, vision, audio, multimodal, and serving tasks.
- Single-source benchmark matrix (`benchmarks/matrix.csv`) + lightweight charts.
- Fixes & Tips index (symptom â†’ fix â†’ verify â†’ scope) for common Hugging Face hurdles.
- MkDocs-powered docs site for quick navigation and sharing.

### Quick Start
```bash
git clone https://github.com/SSusantAchary/Hands-On-Huggingface-AI-Models.git
cd Hands-On-Huggingface-AI-Models
python -m venv .venv && source .venv/bin/activate
pip install -r notebooks/requirements-minimal.txt
```
Open any notebook from `/notebooks` in Jupyter, VS Code, or Colab (badges inside each file).

## ðŸ“’ Notebook Catalog (80 models)
Developer-ready shortlist of Hugging Face models we plan to ship as notebooks. Full catalog refreshes from `/meta/notebook_catalog.csv`.

<!-- CATALOG:START -->
<!-- generated by /scripts/build_catalog.py ; do not edit manually -->
| Notebook | Use case | Deps | Hardware | RAM |
|---|---|---|---|---|
| **AUDIO** Â· [Whisper Tiny](https://huggingface.co/openai/whisper-tiny) | ASR on short clips (EN/multilingual) | transformers, torchaudio, soundfile | CPU/GPU/MLX | <4GB |
| **AUDIO** Â· [Wav2Vec2 Base 960h](https://huggingface.co/facebook/wav2vec2-base-960h) | ASR baseline (LibriSpeech-style) | transformers, torchaudio | CPU/GPU/MLX | 4â€“8GB |
| **AUDIO** Â· [Whisper Base](https://huggingface.co/openai/whisper-base) | Balanced ASR quality vs speed | transformers, torchaudio, soundfile | CPU/GPU/MLX | 4â€“8GB |
| **AUDIO** Â· [Whisper Small](https://huggingface.co/openai/whisper-small) | Improved ASR accuracy | transformers, torchaudio, soundfile | CPU/GPU/MLX | 8â€“16GB |
| **MULTIMODAL** Â· [CLIP ViT-B/32](https://huggingface.co/openai/clip-vit-base-patch32) | Zero-shot image classification & retrieval | transformers | CPU/GPU/MLX | 4â€“8GB |
| **MULTIMODAL** Â· [BLIP Captioning Base](https://huggingface.co/Salesforce/blip-image-captioning-base) | Image captioning on Flickr8k subset | transformers, datasets | CPU/GPU | 8â€“16GB |
| **MULTIMODAL** Â· [BLIP2 OPT 2.7B](https://huggingface.co/Salesforce/blip2-opt-2.7b) | Image captioning with OPT decoder | transformers | CPU/GPU | 16â€“32GB |
| **MULTIMODAL** Â· [CLIP ViT-L/14](https://huggingface.co/laion/CLIP-ViT-L-14) | High quality cross-modal retrieval | transformers | CPU/GPU | 8â€“16GB |
| **NLP** Â· [BERT base uncased](https://huggingface.co/bert-base-uncased) | Sentiment/IMDB baseline (CPU-first pipeline + LoRA TODO) | transformers, datasets, evaluate | CPU/GPU/MLX | <4GB |
| **NLP** Â· [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | Semantic search embeddings (vector DB ready) | sentence-transformers, transformers | CPU/GPU/MLX | <4GB |
| **NLP** Â· [RoBERTa base](https://huggingface.co/roberta-base) | Sentiment on social reviews baseline | transformers, datasets, evaluate | CPU/GPU/MLX | 4â€“8GB |
| **NLP** Â· [DistilBERT base uncased](https://huggingface.co/distilbert-base-uncased) | Real-time sentiment microservice | transformers, datasets | CPU/GPU/MLX | <4GB |
| **VISION** Â· [ViT Base 224](https://huggingface.co/google/vit-base-patch16-224) | Image classification baseline (Imagenette) | transformers, datasets, timm | CPU/GPU/MLX | 4â€“8GB |
| **VISION** Â· [DETR ResNet-50](https://huggingface.co/facebook/detr-resnet-50) | Object detection on sample images | transformers, torchvision | CPU/GPU | 8â€“16GB |
| **VISION** Â· [ResNet-50](https://huggingface.co/microsoft/resnet-50) | Classic classification transfer | torchvision, timm | CPU/GPU/MLX | 4â€“8GB |
| **VISION** Â· [CLIP ViT-B LAION](https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K) | Zero-shot retrieval large corpus | transformers | CPU/GPU | 4â€“8GB |

[View the full 80-model catalog â†’](./notebooks/catalog.md)
<!-- CATALOG:END -->

### Who It's For
- Practitioners who need reproducible, CPU-safe Hugging Face experiments.
- Teams validating Metal or CUDA paths without breaking CPU workflows.
- Contributors adding benchmarks, fixes, or doc polish with low friction.

**Prereqs:** Python â‰¥3.10, git, and `pip`. GPU/Metal optional.

### Fix of the Week
- Metal backend fallback detection with CPU guidance â€“ documented in `/fixes-and-tips/metal-backend-fallback.md`.

### Mini-benchmark of the Week
- CLIP retrieval batch-size sweep scaffolding â€“ see `/docs/benchmarks.md` and run the notebook to populate metrics.

### Cite & License
```
@misc{hands-on-hf,
  author    = {S. Susant Achary},
  title     = {Hands-On Hugging Face AI Models},
  year      = {2025},
  howpublished = {\url{https://github.com/SSusantAchary/Hands-On-Huggingface-AI-Models}}
}
```
Licensed under the [MIT License](LICENSE).

---

Questions or ideas? Open an issue with labels `notebook`, `fix`, `benchmark`, or `apple-silicon` to help us triage fast.
