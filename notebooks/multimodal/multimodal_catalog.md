# Multimodal Notebook Catalog

| Model | Use case | Deps | Hardware | RAM | Notes | Notebook |
|---|---|---|---|---|---|---|
| [CLIP ViT-B/32](https://huggingface.co/openai/clip-vit-base-patch32)<br><sub>openai/clip-vit-base-patch32</sub> | Zero-shot image classification & retrieval | transformers | CPU/GPU/MLX | 4–8GB | Popular baseline; add batching tip | multimodal/multimodal_notebooks/multimodal-01-clip-vit-b-32.ipynb |
| [BLIP Captioning Base](https://huggingface.co/Salesforce/blip-image-captioning-base)<br><sub>Salesforce/blip-image-captioning-base</sub> | Image captioning on Flickr8k subset | transformers, datasets | CPU/GPU | 8–16GB | GPU recommended; CPU works small batch | multimodal/multimodal_notebooks/multimodal-02-blip-captioning-base.ipynb |
| [BLIP2 OPT 2.7B](https://huggingface.co/Salesforce/blip2-opt-2.7b)<br><sub>Salesforce/blip2-opt-2.7b</sub> | Image captioning with OPT decoder | transformers | CPU/GPU | 16–32GB | Use bf16 on GPU; memory hungry | multimodal/multimodal_notebooks/multimodal-03-blip2-opt-2-7b.ipynb |
| [CLIP ViT-L/14](https://huggingface.co/laion/CLIP-ViT-L-14)<br><sub>laion/CLIP-ViT-L-14</sub> | High quality cross-modal retrieval | transformers | CPU/GPU | 8–16GB | Use fp16; large checkpoints | multimodal/multimodal_notebooks/multimodal-04-clip-vit-l-14.ipynb |
| [OWL-ViT Base](https://huggingface.co/google/owlvit-base-patch32)<br><sub>google/owlvit-base-patch32</sub> | Open vocabulary detection | transformers | CPU/GPU | 8–16GB | Prompt carefully; MIT license | multimodal/multimodal_notebooks/multimodal-05-owl-vit-base.ipynb |
| [OWL-ViT Large](https://huggingface.co/google/owlvit-large-patch14)<br><sub>google/owlvit-large-patch14</sub> | Open vocabulary detection high-accuracy | transformers | CPU/GPU | 16–32GB | GPU highly recommended; MIT | multimodal/multimodal_notebooks/multimodal-06-owl-vit-large.ipynb |
| [BLIP VQA Base](https://huggingface.co/Salesforce/blip-vqa-base)<br><sub>Salesforce/blip-vqa-base</sub> | Visual question answering | transformers, datasets | CPU/GPU | 8–16GB | Add answer post-process; MIT | multimodal/multimodal_notebooks/multimodal-07-blip-vqa-base.ipynb |
| [InstructBLIP FLAN-T5 XL](https://huggingface.co/Salesforce/instructblip-flan-t5-xl)<br><sub>Salesforce/instructblip-flan-t5-xl</sub> | Instruction tuned VQA | transformers | CPU/GPU | 16–32GB | Use 4-bit to fit 16GB; license MIT | multimodal/multimodal_notebooks/multimodal-08-instructblip-flan-t5-xl.ipynb |
| [OpenFlamingo 3B](https://huggingface.co/mlfoundations/open_flamingo_3b)<br><sub>mlfoundations/open_flamingo_3b</sub> | Few-shot multimodal generation | transformers | CPU/GPU | 16–32GB | Requires CLIP vision tower; Apache-2.0 | multimodal/multimodal_notebooks/multimodal-09-openflamingo-3b.ipynb |
| [Qwen2 VL 2B](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)<br><sub>Qwen/Qwen2-VL-2B-Instruct</sub> | Compact VLM assistant | transformers | CPU/GPU | 16–32GB | Quantize to 4-bit; license Qwen | multimodal/multimodal_notebooks/multimodal-10-qwen2-vl-2b.ipynb |
| [AltCLIP L-m9](https://huggingface.co/BAAI/AltCLIP-L-m9)<br><sub>BAAI/AltCLIP-L-m9</sub> | Chinese-English CLIP | transformers | CPU/GPU | 8–16GB | Great for cross-lingual retrieval | multimodal/multimodal_notebooks/multimodal-11-altclip-l-m9.ipynb |
| [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)<br><sub>microsoft/kosmos-2-patch14-224</sub> | Vision language grounding | transformers | CPU/GPU | 16–32GB | Needs caption prompts; MIT | multimodal/multimodal_notebooks/multimodal-12-kosmos-2.ipynb |
| [Pix2Struct Base](https://huggingface.co/google/pix2struct-base)<br><sub>google/pix2struct-base</sub> | UI screenshot to text | transformers | CPU/GPU | 8–16GB | Prepare screenshot datasets; MIT | multimodal/multimodal_notebooks/multimodal-13-pix2struct-base.ipynb |
| [BLIP2 FLAN-T5 XL](https://huggingface.co/Salesforce/blip2-flan-t5-xl)<br><sub>Salesforce/blip2-flan-t5-xl</sub> | Vision-language generation | transformers | CPU/GPU | 16–32GB | Heavy; run fp16; MIT | multimodal/multimodal_notebooks/multimodal-14-blip2-flan-t5-xl.ipynb |
| [Idefics2 8B](https://huggingface.co/HuggingFaceM4/idefics2-8b-instruct)<br><sub>HuggingFaceM4/idefics2-8b-instruct</sub> | Large VLM instruction | transformers | CPU/GPU | 32GB+ | Needs multi-GPU or 4-bit; license HF | multimodal/multimodal_notebooks/multimodal-15-idefics2-8b.ipynb |
| [Fuyu 8B](https://huggingface.co/adept/fuyu-8b)<br><sub>adept/fuyu-8b</sub> | OCR aware multimodal chat | transformers | CPU/GPU | 32GB+ | Check Adept license; needs kv cache | multimodal/multimodal_notebooks/multimodal-16-fuyu-8b.ipynb |
| [Stable Diffusion 2.1](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)<br><sub>stabilityai/stable-diffusion-2-1-base</sub> | Text to image generation | diffusers, torchvision | CPU/GPU | 16–32GB | GPU strongly advised; CreativeML license | multimodal/multimodal_notebooks/multimodal-17-stable-diffusion-2-1.ipynb |
| [SDXL Turbo](https://huggingface.co/stabilityai/sdxl-turbo)<br><sub>stabilityai/sdxl-turbo</sub> | Real-time diffusion generation | diffusers | CPU/GPU | 32GB+ | Requires GPU; note non-commercial | multimodal/multimodal_notebooks/multimodal-18-sdxl-turbo.ipynb |
| [LayoutLMv3 Base](https://huggingface.co/microsoft/layoutlmv3-base)<br><sub>microsoft/layoutlmv3-base</sub> | Document question answering | transformers, datasets | CPU/GPU | 8–16GB | Needs layoutlm processor; MIT | multimodal/multimodal_notebooks/multimodal-19-layoutlmv3-base.ipynb |
| [ImageBind 1.2B](https://huggingface.co/facebook/imagebind-1.2b)<br><sub>facebook/imagebind-1.2b</sub> | Unified audio image text embeddings | transformers | CPU/GPU | 16–32GB | Heavy; requires multi modality inputs | multimodal/multimodal_notebooks/multimodal-20-imagebind-1-2b.ipynb |

_Source of truth: `/meta/notebook_catalog.csv`._
