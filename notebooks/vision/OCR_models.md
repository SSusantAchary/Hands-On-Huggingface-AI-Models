# Vision OCR Model Guide

| Model | Primary uses | Highlights | Considerations |
|---|---|---|---|
| [TrOCR Base (Printed)](https://huggingface.co/microsoft/trocr-base-printed)<br><sub>`microsoft/trocr-base-printed`</sub> | Printed documents, receipts, scanned forms | Encoder–decoder built on ViT + GPT2 delivers fast, accurate OCR; solid CPU baseline; good starting point for fine-tuning | English-centric; install `sentencepiece` and `Pillow` for pipelines; longer pages benefit from tiling |
| [TrOCR Base (Handwritten)](https://huggingface.co/microsoft/trocr-base-handwritten)<br><sub>`microsoft/trocr-base-handwritten`</sub> | Handwritten notes, archival materials, signature transcription | Same lightweight architecture tailored for IAM handwriting; strong zero-shot handwriting accuracy | Works best on single-line crops; consider beam search for cursive; GPU recommended for batching |
| [TrOCR Large (Printed)](https://huggingface.co/microsoft/trocr-large-printed)<br><sub>`microsoft/trocr-large-printed`</sub> | High-accuracy OCR on long-form printed pages | Larger vision encoder and decoder improve robustness to noisy scans and small fonts | Heavier (400M params); GPU or MLX strongly preferred; quantization helps for CPU |
| [DiT Base OCR](https://huggingface.co/microsoft/dit-base-finetuned-ocr)<br><sub>`microsoft/dit-base-finetuned-ocr`</sub> | Layout-aware text extraction on complex documents | Transformer on document patches captures layout + text jointly; pairs well with LayoutLMv3 for downstream tasks | Requires image normalization to 512×512 patches; consider mixed precision on GPU |
| [DiT Large DocVQA](https://huggingface.co/microsoft/dit-large-finetuned-docvqa)<br><sub>`microsoft/dit-large-finetuned-docvqa`</sub> | Question answering over OCR’d documents, structured forms | High-capacity model fine-tuned for DocVQA benchmarks; excels at multi-field extraction when combined with prompt-based QA | Memory intensive (>1.2B params); prefer A100/MI200 class GPUs; watch MIT license obligations |
| [Donut Base DocVQA](https://huggingface.co/naver-clova-ix/donut-base-finetuned-docvqa)<br><sub>`naver-clova-ix/donut-base-finetuned-docvqa`</sub> | OCR-free document QA, receipt parsing, multi-field extraction | End-to-end vision encoder + seq2seq decoder predicts structured answers without separate OCR step | Sensitive to input resolution (maintain 960×720 aspect); check CC-BY-NC license for commercial use |
| [Table Transformer (Structure)](https://huggingface.co/microsoft/table-transformer-structure-recognition)<br><sub>`microsoft/table-transformer-structure-recognition`</sub> | Detecting table cells prior to OCR or key-value extraction | DETR-style transformer tuned for table structure; speeds up tabular OCR by isolating columns/cells | Use alongside TrOCR/DiT for cell-level text; output requires post-processing to merge header spans |

## Trending OCR Models (Hugging Face search: `ocr`)

_Snapshot collected via `https://huggingface.co/api/models?limit=50&search=ocr` and sorted by `trendingScore` (UTC 2025-10-14)._

| Model | Trending score | Downloads | Pipeline tag | Notable traits | Notebook / Example |
|---|---|---|---|---|---|
| [nanonets/Nanonets-OCR2-3B](https://huggingface.co/nanonets/Nanonets-OCR2-3B) | 268 | 8.9K | `image-text-to-text` | Fresh Qwen2.5-VL-3B instruct fine-tune for document OCR, PDF→Markdown, multilingual chat-style extraction | [Cookbook – image2md.ipynb](https://github.com/NanoNets/Nanonets-OCR2/blob/main/Nanonets-OCR2-Cookbook/image2md.ipynb) |
| [nanonets/Nanonets-OCR2-1.5B-exp](https://huggingface.co/nanonets/Nanonets-OCR2-1.5B-exp) | 36 | 2.4K | `image-text-to-text` | Lightweight experimental 1.5B variant aimed at multilingual OCR + VQA workflows | [Cookbook – image2md.ipynb](https://github.com/NanoNets/Nanonets-OCR2/blob/main/Nanonets-OCR2-Cookbook/image2md.ipynb) |
| [rednote-hilab/dots.ocr](https://huggingface.co/rednote-hilab/dots.ocr) | 17 | 1.0M | `image-text-to-text` | Popular MIT-licensed DiT-style OCR/VLM covering tables, formulas, layout-rich docs | [Colab remote server demo](https://github.com/rednote-hilab/dots.ocr/blob/master/demo/demo_colab_remote_server.ipynb) |
| [nanonets/Nanonets-OCR-s](https://huggingface.co/nanonets/Nanonets-OCR-s) | 15 | 72K | `image-text-to-text` | Earlier 3B instruct fine-tune still trending due to strong doc QA and conversational extraction | [HF Space demo](https://huggingface.co/spaces/Souvik3333/Nanonets-ocr-s) |
| [mradermacher/Nanonets-OCR2-3B-GGUF](https://huggingface.co/mradermacher/Nanonets-OCR2-3B-GGUF) | 9 | 3.9K | `image-to-text` | GGUF quantization of Nanonets-OCR2-3B for local CPU/GPU inference | [GGUF usage guide (TheBloke)](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF#using-gguf-files) |
| [scb10x/typhoon-ocr-7b](https://huggingface.co/scb10x/typhoon-ocr-7b) | 3 | 24K | `image-to-text` | Qwen2.5-VL 7B tuned for Thai/English document understanding and layout reasoning | [Colab quick start](https://colab.research.google.com/drive/1z4Fm2BZnKcFIoWuyxzzIIIn8oI2GKl3r?usp=sharing) |
| [blackbird/lfm2-vl-furigana-ocr](https://huggingface.co/blackbird/lfm2-vl-furigana-ocr) | 3 | 9 | `text-generation` | LoRA adapter on LFM2-VL targeting Japanese furigana transcription | N/A (no public demo) |
| [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) | 2 | 147K | `image-to-text` | Established ViT-GPT2 OCR for Japanese manga speech bubbles | [README usage guide](https://github.com/kha-white/manga-ocr/blob/master/README.md#usage) |
| [prithivMLmods/Qwen2-VL-OCR-2B-Instruct](https://huggingface.co/prithivMLmods/Qwen2-VL-OCR-2B-Instruct) | 2 | 4.3K | `image-text-to-text` | Math/LaTeX OCR specialty fine-tune on Qwen2-VL-2B | [Colab demo](https://huggingface.co/prithivMLmods/Qwen2-VL-OCR-2B-Instruct/blob/main/Demo/ocrtest_qwen.ipynb) |
| [PaddlePaddle/PP-OCRv5_server_det](https://huggingface.co/PaddlePaddle/PP-OCRv5_server_det) | 2 | 198K | `image-to-text` | Detection component of PaddleOCR v5 server pipeline; pairs with recognition head | [PaddleOCR quick start](https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/module_usage/text_detection.html#iii-quick-start) |
